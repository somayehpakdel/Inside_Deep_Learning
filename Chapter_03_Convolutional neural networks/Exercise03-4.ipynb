{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e2dbd8",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9566157a",
   "metadata": {},
   "source": [
    "<img src='./images/04.png' width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d108f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "os.environ['MLFLOW_TRACKING_URI'] = './mlruns03_4'\n",
    "mlflow.set_tracking_uri(os.environ.get('MLFLOW_TRACKING_URI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73441262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/09 08:42:38 INFO mlflow.tracking.fluent: Experiment with name 'Exercise_1' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/kaggle/working/mlruns/674194671375701503', creation_time=1744188158264, experiment_id='674194671375701503', last_update_time=1744188158264, lifecycle_stage='active', name='Exercise_1', tags={}>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow.set_experiment('Exercise_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645c1960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from utils import train_network, accuracy_score_wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f67aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C = cifar_train[0][0].shape[0]\n",
    "C = 3\n",
    "filter = 16\n",
    "K = 3\n",
    "# w = cifar_train[0][0].shape[1]\n",
    "# h = cifar_train[0][0].shape[2]\n",
    "w , h = 32, 32\n",
    "# classes = cifar_train[0][1].shape[0]\n",
    "classes = 10\n",
    "def build_model(num_conv_layers,\n",
    "                num_pool_layers,\n",
    "                num_hidden_layer=2, \n",
    "                init_hidden_size=512, \n",
    "                decay_factor=2,\n",
    "                activation=nn.ReLU()):\n",
    "    layers =[]\n",
    "    in_channels = C\n",
    "    out_channels = 32\n",
    "    if num_pool_layers:\n",
    "        pool_interval = max(1, num_conv_layers // (num_pool_layers + 1))\n",
    "    else:\n",
    "        pool_interval = num_conv_layers + 1\n",
    "    \n",
    "    currnet_pool_rounds = 0\n",
    "    for i in range(num_conv_layers):\n",
    "        layers.append(nn.Conv2d(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=3//2))\n",
    "        layers.append(activation)\n",
    "        in_channels = out_channels\n",
    "        if (i+1) % pool_interval == 0 and currnet_pool_rounds < num_pool_layers:\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "            currnet_pool_rounds += 1\n",
    "            out_channels *= 2\n",
    "    final_w = w // (2 ** num_pool_layers)\n",
    "    final_h = h // (2 ** num_pool_layers)\n",
    "    fc_layers = []\n",
    "    # Compute the number of features after flattening.\n",
    "    in_features = in_channels * final_w * final_h\n",
    "\n",
    "    fc_layers.append(nn.Flatten())\n",
    "\n",
    "    if num_hidden_layer == 0:\n",
    "        # Directly classify without extra hidden layers.\n",
    "        fc_layers.append(nn.Linear(in_features, classes))\n",
    "    else:\n",
    "        # First FC layer: from flattened output to initial hidden size.\n",
    "        fc_layers.append(nn.Linear(in_features, init_hidden_size))\n",
    "        fc_layers.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Set the current hidden size that will be reduced in subsequent layers.\n",
    "        current_hidden_size = init_hidden_size\n",
    "\n",
    "        # Add additional hidden layers with decreasing size.\n",
    "        for layer in range(1, num_hidden_layer):\n",
    "            # Compute new hidden size with decay.\n",
    "            new_hidden_size = max(10, current_hidden_size // decay_factor)\n",
    "            fc_layers.append(nn.Linear(current_hidden_size, new_hidden_size))\n",
    "            fc_layers.append(nn.ReLU(inplace=True))\n",
    "            current_hidden_size = new_hidden_size\n",
    "\n",
    "        # Final classification layer from the last hidden dimension to the number of classes.\n",
    "        fc_layers.append(nn.Linear(current_hidden_size, classes))\n",
    "\n",
    "    classifier = nn.Sequential(*fc_layers)\n",
    "    model = nn.Sequential(*layers, classifier)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86754e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ac874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data_cifar/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:16<00:00, 10.2MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data_cifar/cifar-10-python.tar.gz to ./data_cifar\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "cifar_train = torchvision.datasets.CIFAR10(\n",
    "    './data_cifar',\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    train=True,)\n",
    "cifar_test = torchvision.datasets.CIFAR10(\n",
    "    './data_cifar',\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    train=False,)\n",
    "\n",
    "cifar_train_loader = DataLoader(\n",
    "    cifar_train,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "cifar_test_loader = DataLoader(\n",
    "    cifar_test,\n",
    "    batch_size=batch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bdc5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_augmentation = {\n",
    "    'RandomAffine1': transforms.RandomAffine(degrees=10),\n",
    "    'RandomAffine2': transforms.RandomAffine(degrees=0, translate=(0.1,0.1)),\n",
    "    'RandomAffine3': transforms.RandomAffine(degrees=0, shear=15),\n",
    "    'RandomHorizontalFlip': transforms.RandomHorizontalFlip(p=0.5),\n",
    "    'RandomVerticalFlip': transforms.RandomVerticalFlip(p=0.2),\n",
    "    'RandomPerspective': transforms.RandomPerspective(p=.2),   \n",
    "    'ColorJitter': transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51608f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_augmented_train_loader = {}\n",
    "for trfm_name, trfm in transforms_augmentation.items():\n",
    "    train_transforms = transforms.Compose(\n",
    "    [trfm,\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]    \n",
    "    )\n",
    "    cifar_augmented_train = torchvision.datasets.CIFAR10(\n",
    "    f'./data_augmented_cifar_{trfm_name}',\n",
    "    download=True,\n",
    "    transform=train_transforms,\n",
    "    train=True,)\n",
    "\n",
    "    cifar_augmented_train_loader[trfm_name] = DataLoader(\n",
    "        cifar_augmented_train,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac804fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "score_funcs = {\"Accuracy\": accuracy_score_wrapper}\n",
    "epochs = 10\n",
    "params = {\n",
    "    'device': device,\n",
    "    'loss_func': loss_func.__class__.__name__,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1f1422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = model_cnn_pool(D=32*32, C=3, filters=128, K=3, classes=10)\n",
    "train_loaders = {'without_aug': cifar_train_loader,\n",
    "                 **cifar_augmented_train_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d79f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for aug_name, train_loader in train_loaders.items():\n",
    "    print(aug_name)\n",
    "    model = build_model(num_conv_layers=4,\n",
    "            num_pool_layers=2,\n",
    "            num_hidden_layer=2, \n",
    "            init_hidden_size=512, \n",
    "            decay_factor=2,\n",
    "            activation=nn.ReLU())\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    params['optimizer'] = optimizer.defaults\n",
    "    \n",
    "    with open('model_summary.txt', 'w') as f:\n",
    "        f.write(str(summary(model)))\n",
    "    with mlflow.start_run(nested=True, run_name=aug_name):\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_artifact('model_summary.txt')\n",
    "\n",
    "        results = train_network(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            loss_func=loss_func,\n",
    "            train_loader=train_loader,\n",
    "            valid_loader=cifar_test_loader,\n",
    "            epochs=epochs,\n",
    "            device=device,\n",
    "            score_funcs=score_funcs,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f71ab6",
   "metadata": {},
   "source": [
    "<img src=\"./images/valid_acc_aug.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea50e288",
   "metadata": {},
   "source": [
    "<img src=\"./images/valid_loss aug.png\">"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
