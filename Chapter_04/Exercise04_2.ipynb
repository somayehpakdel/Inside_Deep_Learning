{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef05e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9e198c",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e661f4",
   "metadata": {},
   "source": [
    "<img src=\"./images/02.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fa20a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_04/utils.py:6: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests, zipfile, io\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import os\n",
    "import mlflow\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import (train_network, roc_auc_score_micro_wrapper, \n",
    "                accuracy_score_wrapper,f1_score_wrapper,\n",
    "                weight_reset, set_seed)\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI'] = './mlruns04_2'\n",
    "mlflow.set_tracking_uri(os.environ.get('MLFLOW_TRACKING_URI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b98d8299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_04/mlruns_2/830193463711506022', creation_time=1749121088476, experiment_id='830193463711506022', last_update_time=1749121088476, lifecycle_stage='active', name='Exercise_2', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment('Exercise_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "799c8108",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_url =  \"https://download.pytorch.org/tutorial/data.zip\"\n",
    "r = requests.get(zip_file_url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()\n",
    "# z = zipfile.ZipFile('./data.zip')\n",
    "# z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e3e98",
   "metadata": {},
   "source": [
    "## Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eca8ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "namge_language_data = {}\n",
    "\n",
    "#We will use some code to remove UNICODE tokens to make life easy for us processing wise\n",
    "#e.g., convert something like \"Ślusàrski\" to Slusarski\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "alphabet = {}\n",
    "for i in range(n_letters):\n",
    "    alphabet[all_letters[i]] = i\n",
    "    \n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99fd4dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageNameDataset_inferred_unicode(Dataset):\n",
    "    def __init__(self, zipfile, vocabulary=None, unicode=False):\n",
    "        self.namge_language_data = {}\n",
    "        self.unicode_or_not(z=zipfile, unicode=unicode)\n",
    "        self.label_names = [x for x in self.namge_language_data.keys()]\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.vocabulary = vocabulary\n",
    "        for y, language in enumerate(self.label_names):\n",
    "            for sample in self.namge_language_data[language]:\n",
    "                self.data.append(sample)\n",
    "                self.labels.append(y)\n",
    "        if vocabulary is None:\n",
    "            vocabulary_set = {char\n",
    "                for names in self.data\n",
    "                for char in names}\n",
    "            vocabulary = {y:x\n",
    "            for x, y in enumerate(vocabulary_set)\n",
    "            }\n",
    "        self.vocabulary = vocabulary\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def string2inputvector(self, input_string):\n",
    "        T = len(input_string)\n",
    "        name_vec = torch.zeros((T), dtype=torch.long)\n",
    "        for pos, character in enumerate(input_string):\n",
    "            name_vec[pos] = self.vocabulary[character]\n",
    "        return name_vec\n",
    "    \n",
    "    def unicode_or_not(self, z, unicode=False):\n",
    "        for zip_path in z.namelist():\n",
    "            if \"data/names/\" in zip_path and zip_path.endswith(\".txt\"):\n",
    "                lang = zip_path[len(\"data/names/\"):-len(\".txt\")]\n",
    "                with z.open(zip_path) as myfile:\n",
    "                    if unicode:\n",
    "                        lang_names = [line.lower() for line in str(myfile.read(), encoding='utf-8').strip().split(\"\\n\")]\n",
    "                    else:\n",
    "                        lang_names = [unicodeToAscii(line).lower() for line in str(myfile.read(), encoding='utf-8').strip().split(\"\\n\")]\n",
    "                    self.namge_language_data[lang] = lang_names\n",
    "                # print(lang, \": \", len(lang_names)) #Print out the name of each language too. \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        name = self.data[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        return self.string2inputvector(name), label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fb95beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "29\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "dataset = LanguageNameDataset_inferred_unicode(zipfile=z, vocabulary=alphabet)\n",
    "dataset_inferred = LanguageNameDataset_inferred_unicode(zipfile=z)\n",
    "dataset_inferred_unicode = LanguageNameDataset_inferred_unicode(zipfile=z, unicode=True)\n",
    "\n",
    "print(len(dataset.vocabulary))\n",
    "print(len(dataset_inferred.vocabulary))\n",
    "print(len(dataset_inferred_unicode.vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d6d205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_pack(batch):\n",
    "    input_tensors = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    for x, y in batch:\n",
    "        input_tensors.append(x)\n",
    "        labels.append(y)\n",
    "        lengths.append(x.shape[0])\n",
    "    x_padded = torch.nn.utils.rnn.pad_sequence(input_tensors, batch_first=False)\n",
    "    x_packed = torch.nn.utils.rnn.pack_padded_sequence(x_padded, lengths, batch_first=False, enforce_sorted=False)\n",
    "    y_batched = torch.as_tensor(labels, dtype=torch.long)\n",
    "    return x_packed, y_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "074cf7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingPackable(nn.Module):\n",
    "    def __init__(self, embed_layer):\n",
    "        super().__init__()\n",
    "        self.embed_layer = embed_layer\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if type(input)== torch.nn.utils.rnn.PackedSequence:\n",
    "            sequences, lengths = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "                input.cpu(),\n",
    "                batch_first=True\n",
    "            )\n",
    "            sequences = self.embed_layer(sequences.to(input.data.device))\n",
    "            return torch.nn.utils.rnn.pack_padded_sequence(\n",
    "                sequences, lengths.cpu(),\n",
    "                batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "        else:\n",
    "            return self.embed_layer(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf366702",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0db9906",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = random_split(\n",
    "    dataset, \n",
    "    (len(dataset)-300, 300),\n",
    "    )\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=pad_and_pack)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=pad_and_pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efe3c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_inferred, test_data_inferred = random_split(\n",
    "    dataset_inferred, \n",
    "    (len(dataset_inferred)-300, 300),\n",
    "    )\n",
    "train_loader_inferred = DataLoader(train_data_inferred, batch_size=batch_size, shuffle=True, collate_fn=pad_and_pack)\n",
    "test_loader_inferred = DataLoader(test_data_inferred, batch_size=batch_size, shuffle=False, collate_fn=pad_and_pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8db39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_inferred_unicode, test_data_inferred_unicode = random_split(\n",
    "    dataset_inferred_unicode, \n",
    "    (len(dataset_inferred_unicode)-300, 300),\n",
    "    )\n",
    "train_loader_inferred_unicode = DataLoader(train_data_inferred_unicode, batch_size=batch_size, shuffle=True, collate_fn=pad_and_pack)\n",
    "test_loader_inferred_unicode = DataLoader(test_data_inferred_unicode, batch_size=batch_size, shuffle=False, collate_fn=pad_and_pack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0838e99d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3de5ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LasttimeStep(nn.Module):\n",
    "    def __init__(self, rnn_layer=1, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.rnn_layer = rnn_layer\n",
    "        if bidirectional:\n",
    "            self.num_bidirectional = 2\n",
    "        else:\n",
    "            self.num_bidirectional = 1\n",
    "    def forward(self, input):\n",
    "        rnn_output = input[0]\n",
    "        last_step = input[1]\n",
    "        if isinstance(last_step, tuple):\n",
    "            last_step = last_step[0]\n",
    "        batch_size = last_step.shape[1]\n",
    "        last_step = last_step.view(self.rnn_layer, self.num_bidirectional, batch_size, -1)\n",
    "        last_step = last_step[-1]\n",
    "        return last_step.reshape(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12ecf386",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 64\n",
    "vocab_size = len(dataset.vocabulary)\n",
    "hidden_nodes = 256\n",
    "classes = len(dataset.label_names)\n",
    "\n",
    "rnn = nn.Sequential(\n",
    "    EmbeddingPackable(nn.Embedding(vocab_size, D)),\n",
    "    nn.RNN(D, hidden_size=hidden_nodes, batch_first=True),\n",
    "    LasttimeStep(),\n",
    "    nn.Linear(hidden_nodes, classes),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1798805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 64\n",
    "vocab_size = len(dataset_inferred.vocabulary)\n",
    "hidden_nodes = 256\n",
    "classes = len(dataset_inferred.label_names)\n",
    "\n",
    "rnn_inferred = nn.Sequential(\n",
    "    EmbeddingPackable(nn.Embedding(vocab_size, D)),\n",
    "    nn.RNN(D, hidden_size=hidden_nodes, batch_first=True),\n",
    "    LasttimeStep(),\n",
    "    nn.Linear(hidden_nodes, classes),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2625c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 64\n",
    "vocab_size = len(dataset_inferred_unicode.vocabulary)\n",
    "hidden_nodes = 256\n",
    "classes = len(dataset_inferred_unicode.label_names)\n",
    "\n",
    "rnn_inferred_unicode = nn.Sequential(\n",
    "    EmbeddingPackable(nn.Embedding(vocab_size, D)),\n",
    "    nn.RNN(D, hidden_size=hidden_nodes, batch_first=True),\n",
    "    LasttimeStep(),\n",
    "    nn.Linear(hidden_nodes, classes),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d5253a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58065066",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "score_funcs = {\"Accuracy\": accuracy_score_wrapper}\n",
    "epochs = 20\n",
    "params = {\n",
    "    'device': device,\n",
    "    'loss_func': loss_func.__class__.__name__,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a60d843",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = (dataset, dataset_inferred, dataset_inferred_unicode)\n",
    "train_loaders = (train_loader ,train_loader_inferred, train_loader_inferred_unicode)\n",
    "test_loaders = (test_loader ,test_loader_inferred, test_loader_inferred_unicode)\n",
    "models = (rnn, rnn_inferred, rnn_inferred_unicode)\n",
    "name_experiment = ('assumed_vocabulary', 'inferred_vocabulary', 'inferred_unicode_vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd48c0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 20/20 [01:33<00:00,  4.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 20/20 [01:04<00:00,  3.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 20/20 [01:14<00:00,  3.75s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 3):\n",
    "    print(i)\n",
    "    model = models[i]\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "    params['optimizer'] = optimizer.defaults\n",
    "    \n",
    "    train_loader = train_loaders[i]\n",
    "    test_loader = test_loaders[i]\n",
    "    params['vocabulary'] = datasets[i].vocabulary\n",
    "    with open('model_summary.txt', 'w') as f:\n",
    "        f.write(str(summary(model)))\n",
    "    with mlflow.start_run(nested=True, run_name=name_experiment[i]):\n",
    "        mlflow.log_artifact('model_summary.txt')\n",
    "        mlflow.log_params(params)\n",
    "    \n",
    "        batch_one_train = train_network(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            loss_func=loss_func,\n",
    "            train_loader=train_loader,\n",
    "            valid_loader=test_loader,\n",
    "            epochs=epochs,\n",
    "            score_funcs=score_funcs,\n",
    "            device=device,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51c33d6",
   "metadata": {},
   "source": [
    "<img src=\"./images/E2_train_acc.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a77406",
   "metadata": {},
   "source": [
    "<img src=\"./images/E2_train_loss.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae63d28",
   "metadata": {},
   "source": [
    "<img src=\"./images/E2_valid_acc.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a00d25",
   "metadata": {},
   "source": [
    "<img src=\"./images/E2_valid_loss.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb594e2",
   "metadata": {},
   "source": [
    "<img src=\"./images/E2_time_epoch.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8aff3",
   "metadata": {},
   "source": [
    "<img src=\"./images/E2_valid_acc_time.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
