{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7af52f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286808ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1586/1799943160.py:3: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('png', 'pdf')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png', 'pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb53d780",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bac5c1",
   "metadata": {},
   "source": [
    "<img src='./images/04.png' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca5c62c",
   "metadata": {},
   "source": [
    "To address the original problem, I modified the `train_simple_network` function to save a version of the model every x epochs. Instead of saving models with different filenames locally, which could potentially fill the hard drive, I store the models in MLflow. By leveraging MLflow, I can keep track of models efficiently without worrying about storage limitations. This approach makes it easy to revisit specific versions of the model and analyze their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28900489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "os.environ['MLFLOW_TRACKING_URI'] = './mlruns'\n",
    "mlflow.set_tracking_uri(os.environ.get('MLFLOW_TRACKING_URI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df1361e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/07 10:31:47 INFO mlflow.tracking.fluent: Experiment with name 'Exercise_4' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_02/mlruns/203838178005775626', creation_time=1744009307431, experiment_id='203838178005775626', last_update_time=1744009307431, lifecycle_stage='active', name='Exercise_4', tags={}>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow.set_experiment('Exercise_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da36465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [00:47<00:00,  1.57s/it]\n",
      "2025/04/07 10:37:07 WARNING mlflow.utils.requirements_utils: Found torch version (2.4.1+cpu) contains a local version label (+cpu). MLflow logged a pip requirement for this package as 'torch==2.4.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/04/07 10:37:13 WARNING mlflow.utils.requirements_utils: Found torch version (2.4.1+cpu) contains a local version label (+cpu). MLflow logged a pip requirement for this package as 'torch==2.4.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import optuna\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import (train_network, accuracy_score_wrapper, \n",
    "                f1_score_wrapper, roc_auc_score_micro_wrapper, \n",
    "                weight_reset, set_seed)\n",
    "from torchinfo import summary\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.types import Schema, TensorSpec\n",
    "from mlflow.models import ModelSignature\n",
    "\n",
    "\n",
    "random_state = 42\n",
    "set_seed(random_state)\n",
    "\n",
    "X_train, Y_train = make_moons(n_samples=8000, noise=0.4, random_state=random_state)\n",
    "X_valid, Y_valid = make_moons(n_samples=200, noise=0.4, random_state=random_state)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                            torch.tensor(Y_train, dtype=torch.long))\n",
    "valid_dataset = TensorDataset(torch.tensor(X_valid, dtype=torch.float32),\n",
    "                            torch.tensor(Y_valid, dtype=torch.long))\n",
    "\n",
    "\n",
    "def plot_results(data_df, close=True):\n",
    "    sns.lineplot(data_df, x='epoch', y='valid AUC', label='valid AUC')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('valid AUC')\n",
    "    plt.title('valid AUC')\n",
    "    fig = plt.gcf()\n",
    "    if close:\n",
    "        plt.close()\n",
    "    return fig\n",
    "    \n",
    "run_id = '219aed50bb7f4f8cbe70429ade80c1a5' #best valid auc from previuos exercise\n",
    "\n",
    "client = MlflowClient()\n",
    "run_data = client.get_run(run_id).data\n",
    "\n",
    "params = run_data.params\n",
    "epochs = 30\n",
    "in_features = 2\n",
    "out_features = 2\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "params['device'] = device\n",
    "params['epochs'] = epochs\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "activation_functions = {\n",
    "'ReLU': nn.ReLU(),\n",
    "'Tanh': nn.Tanh(),\n",
    "'LeakyReLU': nn.LeakyReLU(),\n",
    "'Sigmoid': nn.Sigmoid()\n",
    "}\n",
    "sequential_layer = [\n",
    "    nn.Linear(in_features, int(params['hidden_neurons'])),\n",
    "    activation_functions[params['activation']]\n",
    "]\n",
    "for _ in range(int(params['layers'])):\n",
    "    sequential_layer.append(nn.Linear(int(params['hidden_neurons']), int(params['hidden_neurons'])))\n",
    "    sequential_layer.append(activation_functions[params['activation']])\n",
    "sequential_layer.append(nn.Linear(int(params['hidden_neurons']), out_features))\n",
    "\n",
    "model = nn.Sequential(*sequential_layer)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=float(params['learning_rate']))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=int(params['batch_size']),shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=int(params['batch_size']))\n",
    "\n",
    "with mlflow.start_run(nested=True, run_name='save_every_x_epochs'):\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    \n",
    "    with open (\"model_summary.txt\", \"w\") as f:\n",
    "        f.write(str(summary(model)))\n",
    "    mlflow.log_artifact(\"model_summary.txt\")\n",
    "\n",
    "    fc_results = train_network(\n",
    "        model=model,\n",
    "        loss_func=loss_func,\n",
    "        train_loader=train_dataloader,\n",
    "        valid_loader=valid_dataloader,\n",
    "        epochs=epochs,\n",
    "        optimizer=optimizer,\n",
    "        score_funcs={'Acc':accuracy_score_wrapper, 'F1':f1_score_wrapper, 'AUC':roc_auc_score_micro_wrapper },\n",
    "        device=device,\n",
    "        checkpoint_file_save='model.pth',\n",
    "        checkpoint_every_x=10\n",
    "    )\n",
    "    \n",
    "    input_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 2))])\n",
    "    output_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 2))])\n",
    "    signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "    mlflow.pytorch.log_model(model, \"model\", signature=signature)\n",
    "    mlflow.log_figure(plot_results(fc_results), \"valid_AUC.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d44655",
   "metadata": {},
   "source": [
    "### Without mlflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7169d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.autonotebook import tqdm\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from utils import run_epoch\n",
    "\n",
    "def resum_better_network(model,\n",
    "                        optimizer,\n",
    "                        loss_func,\n",
    "                        train_loader,\n",
    "                        val_loader=None,\n",
    "                        epochs=50,\n",
    "                        device='cpu',\n",
    "                        score_funcs=None,\n",
    "                        checkpoint_file_load=None,\n",
    "                        checkpoint_file_save=None,\n",
    "                        lr=0.001,\n",
    "                        checkpoint_every_x=None,\n",
    "                        ):\n",
    "    model.to(device)\n",
    "    optimizer = optimizer(model.parameters(), lr=lr)\n",
    "\n",
    "    if checkpoint_file_load:\n",
    "        checkpoint = torch.load(checkpoint_file_load,  weights_only=False)\n",
    "        \n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        results = checkpoint['results']\n",
    "        total_train_time = checkpoint['results']['total time'][-1]\n",
    "    else:\n",
    "        results = defaultdict(list)\n",
    "        start_epoch = 0\n",
    "        total_train_time = 0\n",
    "    for epoch in tqdm(range(start_epoch, epochs), desc='Epoch'):\n",
    "        model.train()\n",
    "        \n",
    "        total_train_time += run_epoch(model,\n",
    "                                    optimizer,\n",
    "                                    train_loader,\n",
    "                                    loss_func,\n",
    "                                    device,\n",
    "                                    results,\n",
    "                                    score_funcs,\n",
    "                                    prefix='train',\n",
    "                                    desc='training',\n",
    "                                    )\n",
    "\n",
    "        results['total time'].append(total_train_time)\n",
    "        results['epoch'].append(epoch)\n",
    "        \n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                run_epoch(model,\n",
    "                        optimizer,\n",
    "                        train_loader,\n",
    "                        loss_func,\n",
    "                        device,\n",
    "                        results,\n",
    "                        score_funcs,\n",
    "                        prefix='valid',\n",
    "                        desc='validating',\n",
    "                        )\n",
    "        if checkpoint_every_x and (epoch+1) % checkpoint_every_x == 0:\n",
    "            torch.save(\n",
    "                {\n",
    "                'results': results,\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()\n",
    "                },\n",
    "                f'{checkpoint_file_save.split('.')[0]}_{epoch+1}.pth')\n",
    "\n",
    "    if checkpoint_file_save is not None:\n",
    "        torch.save(\n",
    "            {\n",
    "            'results': results,\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "            },\n",
    "            f'{checkpoint_file_save.split('.')[0]}_final_({epoch+1}).pth')\n",
    "    return pd.DataFrame.from_dict(results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
