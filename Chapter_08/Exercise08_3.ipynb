{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a7b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f982b7",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c94a06",
   "metadata": {},
   "source": [
    "<img src=\"./images/03.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3915b5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_08/utils.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from PIL import Image\n",
    "import mlflow\n",
    "from torchinfo import summary\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from utils import train_network, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0575d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI'] = './mlruns08_3'\n",
    "mlflow.set_tracking_uri(os.environ.get('MLFLOW_TRACKING_URI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc06df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_07/mlruns07_1/143507330168611334', creation_time=1750415411076, experiment_id='143507330168611334', last_update_time=1750415411076, lifecycle_stage='active', name='Exercise07_1', tags={}>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow.set_experiment('Exercise08_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f59cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dfe04c",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e2239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "data_url_zip = \"https://github.com/kamalkraj/DATA-SCIENCE-BOWL-2018/blob/master/data/stage1_train.zip?raw=true\"\n",
    "path = './data/stage1_train'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "resp = urlopen(data_url_zip)\n",
    "zipfile = ZipFile(BytesIO(resp.read()))\n",
    "zipfile.extractall(path=path)\n",
    "paths = glob(path+'/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdab42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSB2018(Dataset):\n",
    "    \"\"\"Dataset class for the 2018 Data Science Bowl.\"\"\"\n",
    "    def __init__(self, paths):\n",
    "        \"\"\"paths: a list of paths to every image folder in the dataset\"\"\"\n",
    "        self.paths = paths\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):   \n",
    "        #There is only one image in each images path. So we will grab the \"first\" thing we find with \"[0]\" at the end\n",
    "        img_path = glob(self.paths[idx] + \"/images/*\")[0]        \n",
    "        #but there are multiple mask images in each mask path\n",
    "        mask_imgs = glob(self.paths[idx] + \"/masks/*\")        \n",
    "        #the image shape is (W, H, 4), the last dimension is an 'alpha' channel that is not used\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor()])\n",
    "        image = transform(Image.open(img_path).convert('RGB'))\n",
    "        masks = [transform(Image.open(mask_path).convert('L')) for mask_path in mask_imgs]\n",
    "        final_mask = masks[0]\n",
    "            # Perform logical OR with subsequent masks\n",
    "        for i in range(1, len(masks)):\n",
    "            final_mask = torch.logical_or(final_mask.bool(), masks[i].bool()).float()\n",
    "        return image, final_mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b8554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dns_data = DSB2018(paths)\n",
    "train_data , test_data = random_split(dns_data, [500, len(dns_data)-500])\n",
    "batch_size = 16\n",
    "train_seg_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_seg_loader = DataLoader(test_data,  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64809262",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d3ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 32\n",
    "C = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e933b89",
   "metadata": {},
   "source": [
    "### Upsample with ConvTranspose2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31926223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_layer(in_filters, out_filters, kernel_size=3):\n",
    "    padding = kernel_size // 2\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_filters, out_filters, kernel_size, padding=padding),\n",
    "        nn.BatchNorm2d(out_filters),\n",
    "        nn.LeakyReLU(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e91b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "convtranspose_model = nn.Sequential(\n",
    "    cnn_layer(C, n_filters), #First layer changes number of channels up to the large numer\n",
    "    cnn_layer(n_filters, n_filters),\n",
    "    nn.MaxPool2d(2), \n",
    "    cnn_layer(n_filters, 2*n_filters),\n",
    "    cnn_layer(2*n_filters, 2*n_filters),\n",
    "    nn.MaxPool2d(2),\n",
    "    cnn_layer(2*n_filters, 4*n_filters),\n",
    "    cnn_layer(4*n_filters, 4*n_filters),\n",
    "    nn.MaxPool2d(2),\n",
    "    cnn_layer(4*n_filters, 8*n_filters),\n",
    "    cnn_layer(8*n_filters, 8*n_filters),\n",
    "    nn.ConvTranspose2d(8*n_filters, 4*n_filters, (3,3), padding=1, output_padding=1, stride=2),\n",
    "    nn.BatchNorm2d(4*n_filters),\n",
    "    nn.LeakyReLU(),\n",
    "    cnn_layer(4*n_filters, 4*n_filters),\n",
    "    nn.ConvTranspose2d(4*n_filters, 2*n_filters, (3,3), padding=1, output_padding=1, stride=2),\n",
    "    nn.BatchNorm2d(2*n_filters),\n",
    "    nn.LeakyReLU(),\n",
    "    cnn_layer(2*n_filters, 2*n_filters),\n",
    "    nn.ConvTranspose2d(2*n_filters, n_filters, (3,3), padding=1, output_padding=1, stride=2),\n",
    "    nn.BatchNorm2d(n_filters),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Conv2d(n_filters, 1, (3,3), padding=1), #Shape is now (B, 1, W, H)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64edbff4",
   "metadata": {},
   "source": [
    "### Upsample with Conv2dExpansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e951657",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dExpansion(nn.Module):\n",
    "    def __init__(self, n_filters_in, n_filters_out, kernel_size, stride=1):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        # Upsample the image by a factor of 2\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest') # or 'bilinear', 'bicubic'\n",
    "        self.conv = nn.Conv2d(n_filters_in, n_filters_out, kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7199cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2dexpansion_model = nn.Sequential(\n",
    "    cnn_layer(C, n_filters), \n",
    "    cnn_layer(n_filters, n_filters),\n",
    "    nn.MaxPool2d(2), \n",
    "    cnn_layer(n_filters, 2*n_filters),\n",
    "    cnn_layer(2*n_filters, 2*n_filters),\n",
    "    nn.MaxPool2d(2),\n",
    "    cnn_layer(2*n_filters, 4*n_filters),\n",
    "    cnn_layer(4*n_filters, 4*n_filters),\n",
    "    nn.MaxPool2d(2),\n",
    "    cnn_layer(4*n_filters, 8*n_filters),\n",
    "    cnn_layer(8*n_filters, 8*n_filters),\n",
    "    Conv2dExpansion(8*n_filters, 4*n_filters, (3,3), stride=1),\n",
    "    nn.BatchNorm2d(4*n_filters),\n",
    "    nn.LeakyReLU(),\n",
    "    cnn_layer(4*n_filters, 4*n_filters),\n",
    "    Conv2dExpansion(4*n_filters, 2*n_filters, (3,3), stride=1),\n",
    "    nn.BatchNorm2d(2*n_filters),\n",
    "    nn.LeakyReLU(),\n",
    "    cnn_layer(2*n_filters, 2*n_filters),\n",
    "    Conv2dExpansion(2*n_filters, n_filters, (3,3), stride=1),\n",
    "    nn.BatchNorm2d(n_filters),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Conv2d(n_filters, 1, (3,3), padding=1), #Shape is now (B, 1, W, H)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477ffb5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af53958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "epochs = 50\n",
    "params = {\n",
    "    'device': device,\n",
    "    'loss_func': loss_func.__class__.__name__,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd7efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(outputs: torch.Tensor, labels: torch.Tensor, smooth=1e-6):\n",
    "    outputs = torch.sigmoid(outputs)\n",
    "    outputs = (outputs > 0.5).float()\n",
    "    outputs = outputs.view(outputs.size(0), -1)  # (B, N_pixels)\n",
    "    labels = labels.view(labels.size(0), -1)    # (B, N_pixels)\n",
    "    intersection = (outputs * labels).sum(dim=1)  # Element-wise product then sum\n",
    "    union = (outputs + labels).sum(dim=1) - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth) # Add smooth to avoid division by zero\n",
    "    return iou.mean()\n",
    "score_funcs = {'iou': iou}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55861dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'convtranspose_model': convtranspose_model,\n",
    "    'conv2dexpansion_model': conv2dexpansion_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f01e095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for experiment, model in models.items():\n",
    "    params['experiment'] = experiment\n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "    params['optimizer'] = optimizer.defaults\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params_all = sum(p.numel() for p in model.parameters())\n",
    "    params['total_params'] = total_params\n",
    "    params['total_params_all'] = total_params_all\n",
    "    with open('model_summary.txt', 'w') as f:\n",
    "        f.write(str(summary(model, inpt_size=(batch_size, C, 28, 28))))\n",
    "    with mlflow.start_run(nested=True, run_name='experiment'):\n",
    "        mlflow.log_artifact('model_summary.txt')\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        results = train_network(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            loss_func=loss_func,\n",
    "            train_loader=train_seg_loader,\n",
    "            valid_loader=test_seg_loader,\n",
    "            epochs=epochs,\n",
    "            device=device,\n",
    "            score_funcs=score_funcs\n",
    "            # checkpoint_file_save='model.pth',\n",
    "            \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4ed78",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb26b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot(models_dict):\n",
    "    \"\"\"Evaluate models on test data and visualize the reconstructions.\"\"\"\n",
    "    # Fetch a single batch of test images\n",
    "    test_images, _ = next(iter(test_loader))\n",
    "    test_images = test_images.to(device)\n",
    "    \n",
    "    reconstructions = {}\n",
    "    for name, model in models_dict.items():\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            reconstructions[name] = model(test_images).cpu()\n",
    "    \n",
    "    # Plotting\n",
    "    n_images_to_show = 8\n",
    "    num_models = len(models_dict)\n",
    "    fig, axes = plt.subplots(num_models + 1, n_images_to_show, figsize=(n_images_to_show * 1.5, (num_models + 1) * 1.5))\n",
    "    \n",
    "    # Plot original images\n",
    "    for i in range(n_images_to_show):\n",
    "        ax = axes[0, i]\n",
    "        ax.imshow(test_images[i].cpu().squeeze(), cmap='gray')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(\"Original\", fontsize=12)\n",
    "            \n",
    "    # Plot reconstructions for each model\n",
    "    row_idx = 1\n",
    "    for name, recon_imgs in reconstructions.items():\n",
    "        for i in range(n_images_to_show):\n",
    "            ax = axes[row_idx, i]\n",
    "            ax.imshow(recon_imgs[i].squeeze(), cmap='gray')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            if i == 0:\n",
    "                ax.set_ylabel(name, fontsize=12)\n",
    "        row_idx += 1\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"Autoencoder Reconstructions Comparison\", fontsize=16, y=1.02)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c1f260",
   "metadata": {},
   "source": [
    "### Expected Differences in Results\n",
    "You might observe the following differences, primarily related to the visual quality of the predicted masks and potentially subtle differences in quantitative metrics like IoU.\n",
    "\n",
    "#### Visual Quality: Checkerboard Artifacts\n",
    "\n",
    "- convtranspose_model: You are more likely to see checkerboard artifacts in the predicted segmentation masks. These manifest as a grid-like pattern of varying pixel intensities, making the edges of the segmented objects look rough or \"blocky.\" This is precisely the issue Conv2dExpansion aims to solve. The artifacts might be more pronounced in areas with fine details or curved boundaries.\n",
    "- conv2dexpansion_model: This model is designed to reduce or eliminate checkerboard artifacts. The explicit nn.Upsample step provides a smoother initial upscaling, and the subsequent nn.Conv2d operates on this already upscaled, uniformly sampled grid. You should expect the predicted masks to have smoother boundaries and less of a grid-like appearance.\n",
    "\n",
    "#### Quantitative Metrics (IoU)\n",
    "\n",
    "It's possible that the IoU (Intersection over Union) metric might be slightly higher or more stable for the conv2dexpansion_model. While the core issue of checkerboard artifacts is visual, these artifacts can sometimes lead to misclassifications at the pixel level, especially at object boundaries. A smoother, less artifact-ridden prediction could translate to a marginally better IoU, as the predicted mask might align more accurately with the ground truth.\n",
    "However, the difference in IoU might be negligible or inconsistent across runs. Deep learning models are complex, and many factors (initialization, exact training dynamics, subtle architectural differences) can influence the final metric. The visual difference is often more striking than the quantitative one when dealing with checkerboard artifacts.\n",
    "\n",
    "\n",
    "#### Training Stability / Convergence\n",
    "\n",
    "It's less common, but sometimes models prone to checkerboard artifacts can have slightly less stable training or converge slower, as the network might struggle to optimize the unevenly distributed information. However, for a well-established architecture like a U-Net, this might not be a major issue, especially if the artifacts are not severe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
