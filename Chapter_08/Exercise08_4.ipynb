{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a7b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f982b7",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c94a06",
   "metadata": {},
   "source": [
    "<img src=\"./images/04.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3915b5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_08/utils.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import mlflow\n",
    "from torchinfo import summary\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from utils import train_network, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0575d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI'] = './mlruns08_4'\n",
    "mlflow.set_tracking_uri(os.environ.get('MLFLOW_TRACKING_URI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc06df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_07/mlruns07_1/143507330168611334', creation_time=1750415411076, experiment_id='143507330168611334', last_update_time=1750415411076, lifecycle_stage='active', name='Exercise07_1', tags={}>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow.set_experiment('Exercise08_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f59cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dfe04c",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e2239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "data_url_zip = \"https://github.com/kamalkraj/DATA-SCIENCE-BOWL-2018/blob/master/data/stage1_train.zip?raw=true\"\n",
    "path = './data/stage1_train'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "resp = urlopen(data_url_zip)\n",
    "zipfile = ZipFile(BytesIO(resp.read()))\n",
    "zipfile.extractall(path=path)\n",
    "paths = glob(path+'/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdab42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSB2018(Dataset):\n",
    "    \"\"\"Dataset class for the 2018 Data Science Bowl.\"\"\"\n",
    "    def __init__(self, paths):\n",
    "        \"\"\"paths: a list of paths to every image folder in the dataset\"\"\"\n",
    "        self.paths = paths\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):   \n",
    "        #There is only one image in each images path. So we will grab the \"first\" thing we find with \"[0]\" at the end\n",
    "        img_path = glob(self.paths[idx] + \"/images/*\")[0]        \n",
    "        #but there are multiple mask images in each mask path\n",
    "        mask_imgs = glob(self.paths[idx] + \"/masks/*\")        \n",
    "        #the image shape is (W, H, 4), the last dimension is an 'alpha' channel that is not used\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor()])\n",
    "        image = transform(Image.open(img_path).convert('RGB'))\n",
    "        masks = [transform(Image.open(mask_path).convert('L')) for mask_path in mask_imgs]\n",
    "        final_mask = masks[0]\n",
    "            # Perform logical OR with subsequent masks\n",
    "        for i in range(1, len(masks)):\n",
    "            final_mask = torch.logical_or(final_mask.bool(), masks[i].bool()).float()\n",
    "        return image, final_mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b8554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dns_data = DSB2018(paths)\n",
    "train_data , test_data = random_split(dns_data, [500, len(dns_data)-500])\n",
    "batch_size = 16\n",
    "train_seg_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_seg_loader = DataLoader(test_data,  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64809262",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d3ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 32\n",
    "C = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17458533",
   "metadata": {},
   "source": [
    "### U-Net with Maxpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a854cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_layer(in_filters, out_filters, kernel_size=3):\n",
    "    \"\"\"\n",
    "    in_filters: how many channels are in the input to this layer\n",
    "    out_filters: how many channels should this layer output\n",
    "    kernel_size: how large should the filters of this layer be\n",
    "    \"\"\"\n",
    "    padding = kernel_size//2\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_filters, out_filters, kernel_size, padding=padding), \n",
    "        nn.BatchNorm2d(out_filters),\n",
    "        nn.LeakyReLU(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7d1849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetBlock2d(nn.Module): #Our class extends nn.Module, all PyTorch layers must extend this \n",
    "    def __init__(self, in_channels, mid_channels, out_channels=None, layers=1, sub_network=None, filter_size=3):\n",
    "        \"\"\" \n",
    "        in_channels: the number of channels in the input to this block\n",
    "        mid_channels: the number of channels to have as the output for each convolutional filter\n",
    "        out_channels: if not `None`, ends the network with a 1x1 convolution to convert the number of output channels to a specific number. \n",
    "        layers: how many blocks of hidden layers to create on both the input and output side of a U-Net block\n",
    "        sub_network: the network to apply after shrinking the input by a factor of 2 using max pooling. The number of output channels should be equal to `mid_channels`\n",
    "        filter_size: how large the convolutional filters should be\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        #Start preparing the layers used to process the input\n",
    "        in_layers = [cnn_layer(in_channels, mid_channels, filter_size)]\n",
    "        \n",
    "        #If we have a sub-network, we will double the number of inputs to the output. So lets figure that out now\n",
    "        if sub_network is None:\n",
    "            inputs_to_outputs = 1\n",
    "        else:\n",
    "            inputs_to_outputs = 2\n",
    "\n",
    "        #Preparing the layers used to make the final output, which has extra input channels from any sub-network\n",
    "        out_layers = [ cnn_layer(mid_channels*inputs_to_outputs, mid_channels, filter_size)]\n",
    "        \n",
    "        #Make the additional hidden layers used for the input and output\n",
    "        for _ in range(layers-1):\n",
    "            in_layers.append(cnn_layer(mid_channels, mid_channels, filter_size))\n",
    "            out_layers.append(cnn_layer(mid_channels, mid_channels, filter_size))\n",
    "        #Use 1x1 Convolutions to ensure a specific output size\n",
    "        if out_channels is not None:\n",
    "            out_layers.append(nn.Conv2d(mid_channels, out_channels, 1, padding=0))\n",
    "    \n",
    "        #define our three total sub-networks:\n",
    "        #1) in_model performs the intial rounds of convolution\n",
    "        self.in_model = nn.Sequential(*in_layers)\n",
    "        #2) our subnetwork works on the max-pooled result. We will add the pooling and up-scaling directly into the sub-model\n",
    "        if sub_network is not None:\n",
    "            self.bottleneck = nn.Sequential(\n",
    "                nn.MaxPool2d(2), #Shrink\n",
    "                sub_network, #process the smaller resolution\n",
    "                #expand back up\n",
    "                nn.ConvTranspose2d(mid_channels, mid_channels, filter_size, padding=filter_size//2, output_padding=1, stride=2)\n",
    "            )\n",
    "        else:\n",
    "            self.bottleneck = None\n",
    "        #3) the output model that processes the concatinated result, or just the output from in_model if no sub-network was given\n",
    "        self.out_model = nn.Sequential(*out_layers)\n",
    "        \n",
    "    \n",
    "    #The forward function is the code that takes an input and produce an output. \n",
    "    def forward(self, x):\n",
    "        #compute the convolutions at the current scale\n",
    "        full_scale_result = self.in_model(x) #(B, C, W, H)\n",
    "        #check if we have a bottleneck to apply\n",
    "        if self.bottleneck is not None:\n",
    "            #(B, C, W, H) shape because bottleneck does both the pooling & expansion\n",
    "            bottle_result = self.bottleneck(full_scale_result)\n",
    "            #Now shape (B, 2*C, W, H)\n",
    "            full_scale_result = torch.cat([full_scale_result, bottle_result], dim=1)\n",
    "        #compute the output on the concatenated (or not!) result\n",
    "        return self.out_model(full_scale_result)\n",
    "#Caption: A class implementing a “Block” of the U-Net approach. Each block needs to know how many channels are coming in and out of the block. The block has three components. 1) the input network, what processes the raw input coming into this block. 2) the bottleneck, which is what the block runs after shrinking the current results down by a factor of 2, and then expands the result back up to the original size. 3) the output network, which is run on the results from the prior two sub-networks concatenated together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bded61",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_with_pooling = nn.Sequential(\n",
    "    UNetBlock2d(3, 32, layers=2, sub_network=\n",
    "        UNetBlock2d(32, 64, out_channels=32, layers=2, sub_network=\n",
    "            UNetBlock2d(64, 128, out_channels=64, layers=2)\n",
    "        ),\n",
    "    ),\n",
    "    #Prediction for _every_ location\n",
    "    nn.Conv2d(32, 1, (3,3), padding=1), #Shape is now (B, 1, W, H)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11fdadc",
   "metadata": {},
   "source": [
    "### U-Net with strided conv (istead of maxpooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd04d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_layer(in_filters, out_filters, kernel_size=3):\n",
    "    \"\"\"\n",
    "    in_filters: how many channels are in the input to this layer\n",
    "    out_filters: how many channels should this layer output\n",
    "    kernel_size: how large should the filters of this layer be\n",
    "    \"\"\"\n",
    "    padding = kernel_size//2\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_filters, out_filters, kernel_size, padding=padding), \n",
    "        nn.BatchNorm2d(out_filters),\n",
    "        nn.LeakyReLU(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec689d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetBlock2d(nn.Module): #Our class extends nn.Module, all PyTorch layers must extend this \n",
    "    def __init__(self, in_channels, mid_channels, out_channels=None, layers=1, sub_network=None, filter_size=3):\n",
    "        \"\"\" \n",
    "        in_channels: the number of channels in the input to this block\n",
    "        mid_channels: the number of channels to have as the output for each convolutional filter\n",
    "        out_channels: if not `None`, ends the network with a 1x1 convolution to convert the number of output channels to a specific number. \n",
    "        layers: how many blocks of hidden layers to create on both the input and output side of a U-Net block\n",
    "        sub_network: the network to apply after shrinking the input by a factor of 2 using max pooling. The number of output channels should be equal to `mid_channels`\n",
    "        filter_size: how large the convolutional filters should be\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        #Start preparing the layers used to process the input\n",
    "        in_layers = [cnn_layer(in_channels, mid_channels, filter_size)]\n",
    "        \n",
    "        #If we have a sub-network, we will double the number of inputs to the output. So lets figure that out now\n",
    "        if sub_network is None:\n",
    "            inputs_to_outputs = 1\n",
    "        else:\n",
    "            inputs_to_outputs = 2\n",
    "\n",
    "        #Preparing the layers used to make the final output, which has extra input channels from any sub-network\n",
    "        out_layers = [ cnn_layer(mid_channels*inputs_to_outputs, mid_channels, filter_size)]\n",
    "        \n",
    "        #Make the additional hidden layers used for the input and output\n",
    "        for _ in range(layers-1):\n",
    "            in_layers.append(cnn_layer(mid_channels, mid_channels, filter_size))\n",
    "            out_layers.append(cnn_layer(mid_channels, mid_channels, filter_size))\n",
    "        #Use 1x1 Convolutions to ensure a specific output size\n",
    "        if out_channels is not None:\n",
    "            out_layers.append(nn.Conv2d(mid_channels, out_channels, 1, padding=0))\n",
    "    \n",
    "        #define our three total sub-networks:\n",
    "        #1) in_model performs the intial rounds of convolution\n",
    "        self.in_model = nn.Sequential(*in_layers)\n",
    "        #2) our subnetwork works on the max-pooled result. We will add the pooling and up-scaling directly into the sub-model\n",
    "        if sub_network is not None:\n",
    "            self.bottleneck = nn.Sequential(\n",
    "                nn.Conv2d(mid_channels, mid_channels, kernel_size=2, stride=2, padding=0), # Downsample\n",
    "                sub_network, #process the smaller resolution\n",
    "                #expand back up\n",
    "                nn.ConvTranspose2d(mid_channels, mid_channels, filter_size, padding=filter_size//2, output_padding=1, stride=2)\n",
    "            )\n",
    "        else:\n",
    "            self.bottleneck = None\n",
    "        #3) the output model that processes the concatinated result, or just the output from in_model if no sub-network was given\n",
    "        self.out_model = nn.Sequential(*out_layers)\n",
    "        \n",
    "    \n",
    "    #The forward function is the code that takes an input and produce an output. \n",
    "    def forward(self, x):\n",
    "        #compute the convolutions at the current scale\n",
    "        full_scale_result = self.in_model(x) #(B, C, W, H)\n",
    "        #check if we have a bottleneck to apply\n",
    "        if self.bottleneck is not None:\n",
    "            #(B, C, W, H) shape because bottleneck does both the pooling & expansion\n",
    "            bottle_result = self.bottleneck(full_scale_result)\n",
    "            #Now shape (B, 2*C, W, H)\n",
    "            full_scale_result = torch.cat([full_scale_result, bottle_result], dim=1)\n",
    "        #compute the output on the concatenated (or not!) result\n",
    "        return self.out_model(full_scale_result)\n",
    "#Caption: A class implementing a “Block” of the U-Net approach. Each block needs to know how many channels are coming in and out of the block. The block has three components. 1) the input network, what processes the raw input coming into this block. 2) the bottleneck, which is what the block runs after shrinking the current results down by a factor of 2, and then expands the result back up to the original size. 3) the output network, which is run on the results from the prior two sub-networks concatenated together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fcf3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_with_strideconv = nn.Sequential(\n",
    "    UNetBlock2d(3, 32, layers=2, sub_network=\n",
    "        UNetBlock2d(32, 64, out_channels=32, layers=2, sub_network=\n",
    "            UNetBlock2d(64, 128, out_channels=64, layers=2)\n",
    "        ),\n",
    "    ),\n",
    "    #Prediction for _every_ location\n",
    "    nn.Conv2d(32, 1, (3,3), padding=1), #Shape is now (B, 1, W, H)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477ffb5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af53958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "epochs = 50\n",
    "params = {\n",
    "    'device': device,\n",
    "    'loss_func': loss_func.__class__.__name__,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd7efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(outputs: torch.Tensor, labels: torch.Tensor, smooth=1e-6):\n",
    "    outputs = torch.sigmoid(outputs)\n",
    "    outputs = (outputs > 0.5).float()\n",
    "    outputs = outputs.view(outputs.size(0), -1)  # (B, N_pixels)\n",
    "    labels = labels.view(labels.size(0), -1)    # (B, N_pixels)\n",
    "    intersection = (outputs * labels).sum(dim=1)  # Element-wise product then sum\n",
    "    union = (outputs + labels).sum(dim=1) - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth) # Add smooth to avoid division by zero\n",
    "    return iou.mean()\n",
    "score_funcs = {'iou': iou}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55861dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'unet_with_pooling': unet_with_pooling,\n",
    "    'unet_with_strideconv': unet_with_strideconv\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f01e095",
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment, model in models.items():\n",
    "    params['experiment'] = experiment\n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "    params['optimizer'] = optimizer.defaults\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params_all = sum(p.numel() for p in model.parameters())\n",
    "    params['total_params'] = total_params\n",
    "    params['total_params_all'] = total_params_all\n",
    "    with open('model_summary.txt', 'w') as f:\n",
    "        f.write(str(summary(model, inpt_size=(batch_size, C, 28, 28))))\n",
    "    with mlflow.start_run(nested=True, run_name='experiment'):\n",
    "        mlflow.log_artifact('model_summary.txt')\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        results = train_network(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            loss_func=loss_func,\n",
    "            train_loader=train_seg_loader,\n",
    "            valid_loader=test_seg_loader,\n",
    "            epochs=epochs,\n",
    "            device=device,\n",
    "            score_funcs=score_funcs\n",
    "            # checkpoint_file_save='model.pth',\n",
    "            \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4ed78",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b1c0d0",
   "metadata": {},
   "source": [
    "- Learnable Downsampling: Strided convolutions introduce learnable parameters into the downsampling process. Instead of simply taking the maximum value (as in max-pooling), the network can learn optimal filters to reduce the spatial dimensions while preserving or even enhancing relevant features for the segmentation task. This allows for a more adaptive and data-driven downsampling.\n",
    "\n",
    "- Information Preservation: Max-pooling is a destructive operation that discards a significant amount of information by only keeping the maximum value in a receptive field. Strided convolutions, being a convolutional operation, process all values in the receptive field, potentially retaining more fine-grained spatial information that can be crucial for precise segmentation boundaries.\n",
    "\n",
    "- Fully Convolutional Nature: By replacing max-pooling with strided convolutions, the U-Net becomes truly fully convolutional from end-to-end. This can lead to more stable training and better performance, as the entire network is optimized through backpropagation without non-parametric operations breaking the flow of gradient learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
