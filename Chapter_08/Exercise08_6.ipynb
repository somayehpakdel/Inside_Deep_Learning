{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a7b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f982b7",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c94a06",
   "metadata": {},
   "source": [
    "<img src=\"./images/06.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3915b5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_08/utils.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from tqdm.autonotebook import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import mlflow\n",
    "from torchinfo import summary\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from utils import train_network, set_seed, weight_reset\n",
    "from typing import DefaultDict, Any, Callable, Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0575d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI'] = './mlruns08_6'\n",
    "mlflow.set_tracking_uri(os.environ.get('MLFLOW_TRACKING_URI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc06df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_07/mlruns07_1/143507330168611334', creation_time=1750415411076, experiment_id='143507330168611334', last_update_time=1750415411076, lifecycle_stage='active', name='Exercise07_1', tags={}>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow.set_experiment('Exercise08_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f59cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dfe04c",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a791e1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954c6a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Class2Detect(Dataset):\n",
    "    \"\"\"This class is used to create a simple converstion of a dataset from a classification problem, to a detection problem. \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, toSample=3, canvas_size=100):\n",
    "        \"\"\"\n",
    "        dataset: the source dataset to sample items from as the \"objects\" to detect\n",
    "        toSample: the maximum number of \"objects\" to put into any image\n",
    "        canvas_size: the width and height of the images to place objects inside of. \n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.toSample = toSample\n",
    "        self.canvas_size = canvas_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        final_size = self.canvas_size\n",
    "        #First, create a larger image that will store all the \"objects\" to detect\n",
    "        img_p = torch.zeros((final_size,final_size), dtype=torch.float32)\n",
    "        #Now we are going to sample up to self.toSample objects to place into the image\n",
    "        for _ in range(np.random.randint(1,self.toSample+1)):\n",
    "            \n",
    "            #Pick an object at random from the original dataset, and its label\n",
    "            img, label = self.dataset[np.random.randint(0,len(self.dataset))]\n",
    "            #Get the height and width of that image\n",
    "            _, img_h, img_w = img.shape\n",
    "            #Pick a random offset of the x and y axis, essentially placing the image at a random location\n",
    "            offsets = np.random.randint(0,final_size-np.max(img.shape),size=(4))\n",
    "            #Change the padding at the end to make sure we come out to a specific 100,100 shape\n",
    "            offsets[1] = final_size - img.shape[1] - offsets[0]\n",
    "            offsets[3] = final_size - img.shape[2] - offsets[2]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                img_p = img_p + F.pad(img, tuple(offsets))\n",
    "            #Lets craete the values for the \"boxes\"\n",
    "            #all of these are in absolute pixel locations\n",
    "            \n",
    "            #x_min determined by the randomly selected offset\n",
    "            xmin = offsets[0]\n",
    "            #x_max is the offset plus the image's width\n",
    "            xmax = offsets[0]+img_w\n",
    "            #y min/max follows the same pattern\n",
    "            ymin = offsets[2]\n",
    "            ymax = offsets[2]+img_h\n",
    "            #now we add to the box with the right label\n",
    "            boxes.append( [xmin, ymin, xmax, ymax] )\n",
    "            labels.append( label )\n",
    "\n",
    "            \n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        return img_p, target\n",
    "#Caption: This class defines a toy MNIST detector. Images from the MNIST dataset are placed at random locations in an image. The object detector will try to learn to predict where digits are, and what digit is at each location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd686bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch is going to contain a python list of objects. In our case, our data loader returns (Tensor, Dict) pairs\n",
    "    The FasterRCNN algorithm wants a List[Tensors] and a List[Dict]. So we will use this function to convert the \n",
    "    batch of data into the form we want, and then give it to the Dataloader to use\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for img, label in batch:\n",
    "        imgs.append(img)\n",
    "        labels.append(label)\n",
    "    return imgs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0352015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Class2Detect(torchvision.datasets.MNIST(\"./data\", train=True, transform=transforms.ToTensor(), download=True))\n",
    "test_data = Class2Detect(torchvision.datasets.MNIST(\"./data\", train=False, transform=transforms.ToTensor(), download=True))\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64809262",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c3cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1\n",
    "classes = 10\n",
    "n_filters = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48961053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_layer(in_filters, out_filters, kernel_size=3):\n",
    "    \"\"\"\n",
    "    in_filters: how many channels are in the input to this layer\n",
    "    out_filters: how many channels should this layer output\n",
    "    kernel_size: how large should the filters of this layer be\n",
    "    \"\"\"\n",
    "    padding = kernel_size//2\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_filters, out_filters, kernel_size, padding=padding), \n",
    "        nn.BatchNorm2d(out_filters),\n",
    "        nn.LeakyReLU(), # I'm not setting the leak value to anything just to make the code shorter. \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d34cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = nn.Sequential(\n",
    "    cnn_layer(C, n_filters),    \n",
    "    cnn_layer(n_filters, n_filters),\n",
    "    cnn_layer(n_filters, n_filters),\n",
    "    nn.MaxPool2d((2,2)),\n",
    "    cnn_layer(n_filters, 2*n_filters),\n",
    "    cnn_layer(2*n_filters, 2*n_filters),\n",
    "    cnn_layer(2*n_filters, 2*n_filters),\n",
    "    nn.MaxPool2d((2,2)),\n",
    "    cnn_layer(2*n_filters, 4*n_filters),\n",
    "    cnn_layer(4*n_filters, 4*n_filters),\n",
    ")\n",
    "#Let Faster RCNN know exactly how many output channels to expect\n",
    "backbone.out_channels = n_filters*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928f5bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many proposals $k$ should be generated? Every aspect ration will be one, and the process will be repeated for multiple image sizes \n",
    "anchor_generator = AnchorGenerator(sizes=((32),), aspect_ratios=((1.0),)) #To make this run faster, we are telling PyTorch to look for only square images that are 32 x 32 in size\n",
    "\n",
    "#Tell PyTorch to use the final output of the backbone as the featuremap (['0']), use adaptive pooling down to a 7x7 grid (output_size=7)\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size=7, sampling_ratio=2)\n",
    "#sampling_ratio is poorly named, and controls details on how the RoI grabs slices of the feature map when a fractional pixel location is predicted (e.g., 5.8 instead of 6). We are not going to go into those low level details, 2 is a reasonable default for most work. \n",
    "        \n",
    "#Now we can create the FasterRCNN object. We give it the backbone network, number of classes, min & max size to process images at (we know all our images at 100 pixels), a mean and standard deviation to subtract from the images, and the anchor generation (RPN) and RoI objects\n",
    "model = FasterRCNN(backbone, num_classes=10, image_mean = [0.5], image_std = [0.229], min_size=100, max_size=100, rpn_anchor_generator=anchor_generator, box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477ffb5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a55e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection import MeanAveragePrecision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af53958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "params = {\n",
    "    'device': device,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f8f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "params['optimizer'] = optimizer.defaults\n",
    "\n",
    "# --- Metric Calculator ---\n",
    "# This object will accumulate predictions and targets to calculate mAP\n",
    "metric_calculator = MeanAveragePrecision(box_format=\"xyxy\")\n",
    "\n",
    "# --- MLflow and Logging Setup ---\n",
    "with open('model_summary.txt', 'w') as f:\n",
    "    # Note: torchinfo's summary is more reliable than torch_summary for these models\n",
    "    f.write(str(summary(model, input_size=(batch_size, C, 100, 100))))\n",
    "\n",
    "with mlflow.start_run(run_name='Faster RCNN with ResNet and Metrics'):\n",
    "    mlflow.log_artifact('model_summary.txt')\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    results = defaultdict(list)\n",
    "    total_train_time = 0\n",
    "\n",
    "    # --- Main Training and Validation Loop ---\n",
    "    for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
    "        results['epoch'].append(epoch)\n",
    "        \n",
    "        # ===================================\n",
    "        #           TRAINING\n",
    "        # ===================================\n",
    "        model.train() # Set model to training mode\n",
    "        running_train_loss = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "            inputs = [img.to(device) for img in inputs]\n",
    "            labels = [{k: v.to(device) for k, v in t.items()} for t in labels]\n",
    "\n",
    "            # Get losses\n",
    "            loss_dict = model(inputs, labels)\n",
    "            loss = sum(l for l in loss_dict.values())\n",
    "            \n",
    "            # Backpropagate\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_train_loss.append(loss.item())\n",
    "\n",
    "        # --- Log Training Results for Epoch ---\n",
    "        total_train_time += (time.time() - start_time)\n",
    "        avg_train_loss = np.mean(running_train_loss)\n",
    "        results['train loss'].append(avg_train_loss)\n",
    "        results['total time'].append(total_train_time)\n",
    "        \n",
    "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
    "        mlflow.log_metric('Time', total_train_time, step=epoch)\n",
    "\n",
    "        # ===================================\n",
    "        #           VALIDATION\n",
    "        # ===================================\n",
    "        if test_loader is not None:\n",
    "            metric_calculator.reset() # Reset metrics before each validation run\n",
    "            running_valid_loss = []\n",
    "            \n",
    "            with torch.no_grad(): # Disable gradient calculations\n",
    "                for inputs, labels in tqdm(test_loader, desc=\"Validating\", leave=False):\n",
    "                    inputs = [img.to(device) for img in inputs]\n",
    "                    labels = [{k: v.to(device) for k, v in t.items()} for t in labels]\n",
    "                    \n",
    "                    # 1. Get validation loss (model in .train() mode)\n",
    "                    model.train()\n",
    "                    loss_dict = model(inputs, labels)\n",
    "                    loss = sum(l for l in loss_dict.values())\n",
    "                    running_valid_loss.append(loss.item())\n",
    "\n",
    "                    # 2. Get predictions for metrics (model in .eval() mode)\n",
    "                    model.eval()\n",
    "                    predictions = model(inputs)\n",
    "                    \n",
    "                    # Update the metric calculator with the new predictions and labels\n",
    "                    metric_calculator.update(predictions, labels)\n",
    "\n",
    "            # --- Log Validation Results for Epoch ---\n",
    "            avg_valid_loss = np.mean(running_valid_loss)\n",
    "            results['valid loss'].append(avg_valid_loss)\n",
    "            mlflow.log_metric(\"valid_loss\", avg_valid_loss, step=epoch)\n",
    "\n",
    "            # Compute and log the final mAP scores\n",
    "            metrics = metric_calculator.compute()\n",
    "            results['valid_mAP'].append(metrics['map'].item())\n",
    "            mlflow.log_metric(\"valid_mAP\", metrics['map'].item(), step=epoch)\n",
    "            mlflow.log_metric(\"valid_mAP_50\", metrics['map_50'].item(), step=epoch)\n",
    "            mlflow.log_metric(\"valid_mAP_75\", metrics['map_75'].item(), step=epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Valid Loss: {avg_valid_loss:.4f} | mAP: {metrics['map'].item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
