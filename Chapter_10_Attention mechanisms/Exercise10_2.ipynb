{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "551ad63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8408d1",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f30180",
   "metadata": {},
   "source": [
    "<img src=\"./images/02.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6883b089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_10/utils.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "import wandb\n",
    "from torchinfo import summary\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import train_network, set_seed, accuracy_score_wrapper, weight_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0838f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "415bcab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51c91a6",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6592df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = torchvision.datasets.MNIST(\"./data\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = torchvision.datasets.MNIST(\"./data\", train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55e02b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargestDigitVariable(Dataset):\n",
    "    def __init__(self, dataset, max_to_sample=6):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.max_to_sample = max_to_sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, index):\n",
    "        how_many = np.random.randint(1, self.max_to_sample, size=1)[0]\n",
    "        selected = np.random.randint(1, len(self.dataset), size=how_many)\n",
    "        x_new = torch.stack(\n",
    "            [self.dataset[i][0] for i in selected]\n",
    "            + [torch.zeros((1, 28, 28)) for i in range(self.max_to_sample-how_many)])\n",
    "        y_new = max([self.dataset[i][1] for i in selected])\n",
    "        return x_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805b24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 128\n",
    "\n",
    "largest_train = LargestDigitVariable(mnist_train)\n",
    "largest_test = LargestDigitVariable(mnist_test)\n",
    "T = largest_train.max_to_sample\n",
    "train_loader = DataLoader(largest_train, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(largest_test, batch_size=B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e28a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_fill(x, time_dimension=1, fill=0):\n",
    "    \"\"\"\n",
    "\n",
    "    :param x: (B, ..., T, ...)\n",
    "    :type x: tensor\n",
    "    :param time_dimension: time dimention of x, defaults to 1\n",
    "    :type time_dimension: int, optional\n",
    "    :param fill: the constand use dto denote that an item, defaults to 0\n",
    "    :type fill: int, optional\n",
    "    \"\"\"\n",
    "    dimensions_to_sum_over = list(range(1, len(x.shape)))\n",
    "    if time_dimension in dimensions_to_sum_over:\n",
    "        dimensions_to_sum_over.remove(time_dimension)\n",
    "    with torch.no_grad():\n",
    "        mask = torch.sum((x != fill), dim=dimensions_to_sum_over) > 0\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03839d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotScore(nn.Module):\n",
    "\n",
    "    def __init__(self, H):\n",
    "        \"\"\"\n",
    "        H: the number of dimensions coming into the dot score. \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.H = H\n",
    "    \n",
    "    def forward(self, states, context):\n",
    "        \"\"\"\n",
    "        states: (B, T, H) shape\n",
    "        context: (B, H) shape\n",
    "        output: (B, T, 1), giving a score to each of the T items based on the context \n",
    "        \n",
    "        \"\"\"\n",
    "        T = states.size(1)\n",
    "        #compute $\\boldsymbol{h}_t^\\top \\bar{\\boldsymbol{h}}$\n",
    "        scores = torch.bmm(states,context.unsqueeze(2)) / np.sqrt(self.H) #(B, T, H) -> (B, T, 1)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547ae6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralScore(nn.Module):\n",
    "    def __init__(self, H, init_method='random'):\n",
    "        \"\"\"\n",
    "        H: The number of dimensions.\n",
    "        init_method: 'random' (default) or 'dotscore_like'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.w = nn.Bilinear(H, H, 1)\n",
    "\n",
    "        if init_method == 'dotscore_like':\n",
    "            with torch.no_grad():\n",
    "                identity_scaled = torch.eye(H).unsqueeze(0) / np.sqrt(H)\n",
    "                epsilon = torch.empty_like(identity_scaled).uniform_(-.01, .01)\n",
    "                self.w.weight.copy_(identity_scaled + epsilon)\n",
    "        elif init_method != 'random':\n",
    "            raise ValueError(f\"Unknown init_method: {init_method}\")\n",
    "    \n",
    "    def forward(self, states, context):\n",
    "        \"\"\"\n",
    "        states: (B, T, H) shape\n",
    "        context: (B, H) shape\n",
    "        output: (B, T, 1), giving a score to each of the T items based on the context \n",
    "        \n",
    "        \"\"\"\n",
    "        T = states.size(1)\n",
    "        #Repeating the values T times \n",
    "        context = torch.stack([context for _ in range(T)], dim=1) #(B, H) -> (B, T, H)\n",
    "        scores = self.w(states, context) #(B, T, H) -> (B, T, 1)\n",
    "        return scores        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64937a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplyAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This helper module is used to apply the results of an attention mechanism toa set of inputs. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, states, attention_scores, mask=None):\n",
    "        \"\"\"\n",
    "        states: (B, T, H) shape giving the T different possible inputs\n",
    "        attention_scores: (B, T, 1) score for each item at each context\n",
    "        mask: None if all items are present. Else a boolean tensor of shape \n",
    "            (B, T), with `True` indicating which items are present / valid. \n",
    "            \n",
    "        returns: a tuple with two tensors. The first tensor is the final context\n",
    "        from applying the attention to the states (B, H) shape. The second tensor\n",
    "        is the weights for each state with shape (B, T, 1). \n",
    "        \"\"\"\n",
    "        \n",
    "        if mask is not None:\n",
    "            #set everything not present to a large negative value that will cause vanishing gradients \n",
    "            attention_scores[~mask] = -1000.0\n",
    "        #compute the weight for each score\n",
    "        weights = F.softmax(attention_scores, dim=1) #(B, T, 1) still, but sum(T) = 1\n",
    "    \n",
    "        final_context = (states*weights).sum(dim=1) #(B, T, D) * (B, T, 1) -> (B, D)\n",
    "        return final_context, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d989e062",
   "metadata": {},
   "source": [
    "### Backbone: Fully_Connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec945593",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten2(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes a vector of shape (A, B, C, D, E, ...)\n",
    "    and flattens everything but the first two dimensions, \n",
    "    giving a result of shape (A, B, C*D*E*...)\n",
    "    \"\"\"\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), input.size(1), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d36a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmarterAttentionNetFC(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, out_size, score_net=None):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            Flatten2(),# Shape is now (B, T, D)\n",
    "            nn.Linear(input_size,hidden_size), #Shape becomes (B, T, H)\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "        )#returns (B, T, H)\n",
    "        \n",
    "        #Try changing this and see how the results change!\n",
    "        self.score_net = score_net\n",
    "\n",
    "        self.apply_attn = ApplyAttention()\n",
    "        \n",
    "        self.prediction_net = nn.Sequential( #(B, H), \n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Linear(hidden_size, out_size ) #(B, H)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "\n",
    "        mask = get_mask_fill(input)\n",
    "\n",
    "        h = self.backbone(input) #(B, T, D) -> (B, T, H)\n",
    "\n",
    "        #h_context = torch.mean(h, dim=1) \n",
    "        #computes torch.mean but ignoring the masked out parts\n",
    "        #first add together all the valid items\n",
    "        h_context = (mask.unsqueeze(-1)*h).sum(dim=1)#(B, T, H) -> (B, H)\n",
    "        #then divide by the number of valid items, pluss a small value incase a bag was all empty\n",
    "        h_context = h_context/(mask.sum(dim=1).unsqueeze(-1)+1e-10)\n",
    "\n",
    "        scores = self.score_net(h, h_context) # (B, T, H) , (B, H) -> (B, T, 1)\n",
    "\n",
    "        final_context, _ = self.apply_attn(h, scores, mask=mask)\n",
    "\n",
    "        return self.prediction_net(final_context)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5726f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = 256\n",
    "classes = 10\n",
    "D = 28 * 28\n",
    "attn_dc = SmarterAttentionNetFC(D, neurons, classes, score_net=DotScore(neurons))\n",
    "attn_gc = SmarterAttentionNetFC(D, neurons, classes, score_net=GeneralScore(neurons, init_method='random'))\n",
    "attn_gcinit = SmarterAttentionNetFC(D, neurons, classes, score_net=GeneralScore(neurons, init_method='dotscore_like'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb0f4d9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43725e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "score_funcs = {\"Accuracy\": accuracy_score_wrapper}\n",
    "epochs = 10\n",
    "config = {\n",
    "    'device': device,\n",
    "    'loss_func': loss_func.__class__.__name__,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': B,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae73b515",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'dot_score': attn_dc,\n",
    "    'general_score': attn_gc,\n",
    "    'general_score_init': attn_gcinit,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2191de28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [00:33<00:00, 33.98s/it]\n"
     ]
    }
   ],
   "source": [
    "for experiment, model in models.items():\n",
    "    print(experiment)\n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "    config['optimizer'] = optimizer.defaults\n",
    "    with open('model_summary.txt', 'w') as f:\n",
    "        f.write(str(summary(model, inpt_size=(B, T, 1, 28, 28))))\n",
    "    wandb.init(\n",
    "        project=\"Exercise10_2\",\n",
    "        name=experiment,\n",
    "        config=config\n",
    "    )\n",
    "    artifact = wandb.Artifact('model_summary', type='model_architecture')\n",
    "    artifact.add_file('model_summary.txt')\n",
    "    wandb.log_artifact(artifact)\n",
    "\n",
    "    results = train_network(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss_func=loss_func,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        epochs=epochs,\n",
    "        device=device,\n",
    "        score_funcs=score_funcs          \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
