{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb176cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97e7875",
   "metadata": {},
   "source": [
    "# Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ea9880",
   "metadata": {},
   "source": [
    "<img src=\"./images/07.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd770c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_04/utils.py:6: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests, zipfile, io\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import os\n",
    "import mlflow\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import (train_network, roc_auc_score_micro_wrapper, \n",
    "                accuracy_score_wrapper,f1_score_wrapper,\n",
    "                weight_reset, set_seed)\n",
    "from torchinfo import summary\n",
    "from  sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2db42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI'] = './mlruns06_7'\n",
    "mlflow.set_tracking_uri(os.environ.get('MLFLOW_TRACKING_URI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768cc5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/07 09:32:51 INFO mlflow.tracking.fluent: Experiment with name 'Exercise_7' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_04/mlruns_7/451609404090313110', creation_time=1749276171051, experiment_id='451609404090313110', last_update_time=1749276171051, lifecycle_stage='active', name='Exercise_7', tags={}>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow.set_experiment('Exercise06_7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8062f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_url =  \"https://download.pytorch.org/tutorial/data.zip\"\n",
    "r = requests.get(zip_file_url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()\n",
    "# z = zipfile.ZipFile('./data.zip')\n",
    "# z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369722e7",
   "metadata": {},
   "source": [
    "## Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41e5070",
   "metadata": {},
   "outputs": [],
   "source": [
    "namge_language_data = {}\n",
    "\n",
    "#We will use some code to remove UNICODE tokens to make life easy for us processing wise\n",
    "#e.g., convert something like \"Ślusàrski\" to Slusarski\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "alphabet = {}\n",
    "for i in range(n_letters):\n",
    "    alphabet[all_letters[i]] = i\n",
    "    \n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af01e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageNameDataset_inferred_unicode(Dataset):\n",
    "    def __init__(self, zipfile, vocabulary=None, unicode=False):\n",
    "        self.namge_language_data = {}\n",
    "        self.unicode_or_not(z=zipfile, unicode=unicode)\n",
    "        self.label_names = [x for x in self.namge_language_data.keys()]\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.vocabulary = vocabulary\n",
    "        for y, language in enumerate(self.label_names):\n",
    "            for sample in self.namge_language_data[language]:\n",
    "                self.data.append(sample)\n",
    "                self.labels.append(y)\n",
    "        if vocabulary is None:\n",
    "            vocabulary_set = {char\n",
    "                for names in self.data\n",
    "                for char in names}\n",
    "            vocabulary = {y:x\n",
    "            for x, y in enumerate(vocabulary_set)\n",
    "            }\n",
    "        self.vocabulary = vocabulary\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def string2inputvector(self, input_string):\n",
    "        T = len(input_string)\n",
    "        name_vec = torch.zeros((T), dtype=torch.long)\n",
    "        for pos, character in enumerate(input_string):\n",
    "            name_vec[pos] = self.vocabulary[character]\n",
    "        return name_vec\n",
    "    \n",
    "    def unicode_or_not(self, z, unicode=False):\n",
    "        for zip_path in z.namelist():\n",
    "            if \"data/names/\" in zip_path and zip_path.endswith(\".txt\"):\n",
    "                lang = zip_path[len(\"data/names/\"):-len(\".txt\")]\n",
    "                with z.open(zip_path) as myfile:\n",
    "                    if unicode:\n",
    "                        lang_names = [line.lower() for line in str(myfile.read(), encoding='utf-8').strip().split(\"\\n\")]\n",
    "                    else:\n",
    "                        lang_names = [unicodeToAscii(line).lower() for line in str(myfile.read(), encoding='utf-8').strip().split(\"\\n\")]\n",
    "                    self.namge_language_data[lang] = lang_names\n",
    "                # print(lang, \": \", len(lang_names)) #Print out the name of each language too. \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        name = self.data[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        return self.string2inputvector(name), label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17fdf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "dataset = LanguageNameDataset_inferred_unicode(zipfile=z)\n",
    "print(len(dataset.vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cb2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, validation_idx = train_test_split(np.arange(len(dataset)),\n",
    "                                            test_size=0.1,\n",
    "                                            random_state=999,\n",
    "                                            shuffle=True,\n",
    "                                            stratify=dataset.labels)\n",
    "\n",
    "# Subset dataset for train and val\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "test_dataset = Subset(dataset, validation_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b783cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_pack(batch):\n",
    "    input_tensors = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    for x, y in batch:\n",
    "        input_tensors.append(x)\n",
    "        labels.append(y)\n",
    "        lengths.append(x.shape[0])\n",
    "    x_padded = torch.nn.utils.rnn.pad_sequence(input_tensors, batch_first=False)\n",
    "    x_packed = torch.nn.utils.rnn.pack_padded_sequence(x_padded, lengths, batch_first=False, enforce_sorted=False)\n",
    "    y_batched = torch.as_tensor(labels, dtype=torch.long)\n",
    "    return x_packed, y_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a919bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_and_pack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bdafc8",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bebf6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LasttimeStep(nn.Module):\n",
    "    def __init__(self, rnn_layer=1, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.rnn_layer = rnn_layer\n",
    "        if bidirectional:\n",
    "            self.num_bidirectional = 2\n",
    "        else:\n",
    "            self.num_bidirectional = 1\n",
    "    def forward(self, input):\n",
    "        rnn_output = input[0]\n",
    "        last_step = input[1]\n",
    "        if isinstance(last_step, tuple):\n",
    "            last_step = last_step[0]\n",
    "        batch_size = last_step.shape[1]\n",
    "        last_step = last_step.view(self.rnn_layer, self.num_bidirectional, batch_size, -1)\n",
    "        last_step = last_step[-1]\n",
    "        return last_step.reshape(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d0bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingPackable(nn.Module):\n",
    "    def __init__(self, embed_layer):\n",
    "        super().__init__()\n",
    "        self.embed_layer = embed_layer\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if type(input)== torch.nn.utils.rnn.PackedSequence:\n",
    "            sequences, lengths = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "                input.cpu(),\n",
    "                batch_first=True\n",
    "            )\n",
    "            sequences = self.embed_layer(sequences.to(input.data.device))\n",
    "            return torch.nn.utils.rnn.pack_padded_sequence(\n",
    "                sequences, lengths.cpu(),\n",
    "                batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "        else:\n",
    "            return self.embed_layer(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc703057",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 64\n",
    "vocab_size = len(dataset.vocabulary)\n",
    "hidden_nodes = 256\n",
    "classes = len(dataset.label_names)\n",
    "\n",
    "def rnn_model(model_type, num_layers, hidden_size):\n",
    "    return nn.Sequential(\n",
    "        EmbeddingPackable(nn.Embedding(vocab_size, D)),\n",
    "        model_type(D, hidden_size=hidden_size, batch_first=True, num_layers=num_layers),\n",
    "        LasttimeStep(rnn_layer=num_layers),\n",
    "        nn.Linear(hidden_size, classes),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33b589b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7ced04",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "score_funcs = {\"Accuracy\": accuracy_score_wrapper}\n",
    "epochs = 5\n",
    "params = {\n",
    "    'device': device,\n",
    "    'loss_func': loss_func.__class__.__name__,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63fe479",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = {\n",
    "    'lstm': nn.LSTM,\n",
    "    'gru': nn.GRU\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f7678b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 3/5 [18:24<12:16, 368.06s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m\n",
      "\u001b[1;32m     12\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mlog_artifact(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_summary.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;32m     13\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mlog_params(params)\n",
      "\u001b[0;32m---> 15\u001b[0m batch_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscore_funcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_funcs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     24\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_04/utils.py:108\u001b[0m, in \u001b[0;36mtrain_network\u001b[0;34m(model, loss_func, train_loader, valid_loader, test_loader, epochs, device, score_funcs, checkpoint_file_save, checkpoint_file_load, lr_schedule, optimizer, disable_tqdm, checkpoint_every_x)\u001b[0m\n",
      "\u001b[1;32m    104\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;32m    105\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(epoch)\n",
      "\u001b[1;32m    106\u001b[0m total_train_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m run_epoch(\n",
      "\u001b[1;32m    107\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n",
      "\u001b[0;32m--> 108\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n",
      "\u001b[1;32m    109\u001b[0m     data_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n",
      "\u001b[1;32m    110\u001b[0m     loss_func\u001b[38;5;241m=\u001b[39mloss_func,\n",
      "\u001b[1;32m    111\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n",
      "\u001b[1;32m    112\u001b[0m     results\u001b[38;5;241m=\u001b[39mresults,\n",
      "\u001b[1;32m    113\u001b[0m     score_funcs\u001b[38;5;241m=\u001b[39mscore_funcs,\n",
      "\u001b[1;32m    114\u001b[0m     prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n",
      "\u001b[1;32m    115\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;32m    116\u001b[0m )\n",
      "\u001b[1;32m    117\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal time\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(total_train_time)\n",
      "\u001b[1;32m    118\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain loss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],step\u001b[38;5;241m=\u001b[39mepoch)\n",
      "\n",
      "File \u001b[0;32m~/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_04/utils.py:44\u001b[0m, in \u001b[0;36mrun_epoch\u001b[0;34m(model, optimizer, data_loader, loss_func, device, results, score_funcs, prefix, desc, disable_tqdm)\u001b[0m\n",
      "\u001b[1;32m     42\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;32m     43\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;32m---> 44\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;32m     45\u001b[0m running_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score_funcs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.12/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n",
      "\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n",
      "\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n",
      "\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n",
      "\u001b[1;32m    520\u001b[0m     )\n",
      "\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n",
      "\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n",
      "\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n",
      "\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n",
      "\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n",
      "\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.12/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n",
      "\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n",
      "\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n",
      "\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n",
      "\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for experiment, model_type in model_types.items():\n",
    "    print(experiment)\n",
    "    for num_layers in range(1, 3):\n",
    "        print('num_layers', num_layers)\n",
    "        for hidden_size in [64, 128, 256]:\n",
    "            print('hidden_size', hidden_size)\n",
    "            model = rnn_model(model_type, num_layers, hidden_size)\n",
    "            optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "            params['optimizer'] = optimizer.defaults\n",
    "            params['vocabulary'] = dataset.vocabulary\n",
    "            params['experiment'] = experiment\n",
    "            params['num_layers'] = num_layers\n",
    "            params['hidden_size'] = hidden_size\n",
    "            with open('model_summary.txt', 'w') as f:\n",
    "                f.write(str(summary(model)))\n",
    "            with mlflow.start_run(nested=True, run_name=experiment+f'_{num_layers}_{hidden_size}'):\n",
    "                mlflow.log_artifact('model_summary.txt')\n",
    "                mlflow.log_params(params)\n",
    "            \n",
    "                batch_train = train_network(\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    loss_func=loss_func,\n",
    "                    train_loader=train_loader,\n",
    "                    valid_loader=test_loader,\n",
    "                    epochs=epochs,\n",
    "                    score_funcs=score_funcs,\n",
    "                    device=device,\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
