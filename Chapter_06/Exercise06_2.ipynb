{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d804a17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2150797",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3ee68",
   "metadata": {},
   "source": [
    "<img src=\"./images/02.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50242734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_06/utils.py:6: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import mlflow\n",
    "from torchinfo import summary\n",
    "from utils import train_network, accuracy_score_wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8cbc5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI'] = './mlruns06_2'\n",
    "mlflow.set_tracking_uri(os.environ.get('MLFLOW_TRACKING_URI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2110b0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/08 15:29:31 INFO mlflow.tracking.fluent: Experiment with name 'Exercise06_2' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_06/mlruns06_2/743342736727177787', creation_time=1749383971503, experiment_id='743342736727177787', last_update_time=1749383971503, lifecycle_stage='active', name='Exercise06_2', tags={}>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment('Exercise06_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36bcf390",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic=True\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d527bb9f",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dda8bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.FashionMNIST(\"./\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = torchvision.datasets.FashionMNIST(\"./\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bda7413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = train_data.data.shape[1] \n",
    "H = train_data.data.shape[2]\n",
    "D = W * H\n",
    "C = 1\n",
    "classes = len(torch.unique(train_data.targets))\n",
    "n_filters = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eb294a",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1bb6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_layer(in_filters, out_filters=None, kernel_size=3, activation=nn.LeakyReLU(.1)):\n",
    "    if out_filters is None:\n",
    "        out_filters = in_filters\n",
    "    padding = kernel_size // 2\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_filters, out_filters, kernel_size, padding=padding),\n",
    "        activation\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fe199e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_layer_bn(in_filters, out_filters=None, kernel_size=3, activation=nn.LeakyReLU(.1)):\n",
    "    if out_filters is None:\n",
    "        out_filters = in_filters\n",
    "    padding = kernel_size // 2\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_filters, out_filters, kernel_size, padding=padding),\n",
    "        nn.BatchNorm2d(out_filters),\n",
    "        activation\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65e10aa",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "867b972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "score_funcs = {\"Accuracy\": accuracy_score_wrapper}\n",
    "epochs = 10\n",
    "params = {\n",
    "    'device': device,\n",
    "    'loss_func': loss_func.__class__.__name__,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "517c8436",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_layer = {\n",
    "    'cnn': cnn_layer,\n",
    "    'cnn_bn': cnn_layer_bn,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bf80c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment, one_layer in type_layer.items():\n",
    "    for num_layers in range(1, 21):  \n",
    "        print(f\"{num_layers} CNN layers ---\")\n",
    "\n",
    "        layers = []\n",
    "        in_channels = C\n",
    "        num_maxpool = 0  # Track the number of max pooling layers added\n",
    "        cnn_layer_count = 0\n",
    "\n",
    "        # Calculate the interval for adding max pooling layers\n",
    "        pooling_interval = max(1, num_layers // 3)  # Ensure at least 1\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            # Add a CNN layer\n",
    "            layers.append(one_layer(in_channels, n_filters))\n",
    "            in_channels = n_filters\n",
    "            cnn_layer_count += 1\n",
    "\n",
    "            # Add MaxPool2d if conditions are met\n",
    "            if num_layers > 3 and num_maxpool < 3 and cnn_layer_count >= pooling_interval:\n",
    "                layers.append(nn.MaxPool2d((2, 2)))\n",
    "                num_maxpool += 1\n",
    "                cnn_layer_count = 0 #reset counter\n",
    "\n",
    "        # Adaptive Pooling and Flatten\n",
    "        adaptive_size = 8\n",
    "        layers.append(nn.AdaptiveAvgPool2d((adaptive_size, adaptive_size)))\n",
    "        layers.append(nn.Flatten())\n",
    "\n",
    "        # Calculate the size of the input to the linear layer\n",
    "        flattened_size = n_filters * adaptive_size * adaptive_size\n",
    "        layers.append(nn.Linear(flattened_size, classes))\n",
    "\n",
    "        # Create the model\n",
    "        model = nn.Sequential(*layers)\n",
    "        optimizer = torch.optim.AdamW(model.parameters())\n",
    "        params['optimizer'] = optimizer.defaults\n",
    "        params['num_layers'] = num_layers\n",
    "        params['experiment'] = experiment\n",
    "        with open('model_summary.txt', 'w') as f:\n",
    "            f.write(str(summary(model, input_size=(batch_size, C, W, H))))\n",
    "        with mlflow.start_run(nested=True, run_name=experiment+f'{num_layers}'):\n",
    "            mlflow.log_artifact('model_summary.txt')\n",
    "            mlflow.log_params(params)\n",
    "            fc_results = train_network(\n",
    "                model=model,\n",
    "                loss_func=loss_func,\n",
    "                train_loader=train_loader,\n",
    "                valid_loader=test_loader,\n",
    "                # test_loader=test_loader,\n",
    "                epochs=epochs,\n",
    "                optimizer=optimizer,\n",
    "                score_funcs=score_funcs,\n",
    "                device=device,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8eb86dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 CNN layers ---\n",
      "2 CNN layers ---\n",
      "3 CNN layers ---\n",
      "4 CNN layers ---\n",
      "5 CNN layers ---\n",
      "6 CNN layers ---\n",
      "7 CNN layers ---\n",
      "8 CNN layers ---\n",
      "9 CNN layers ---\n",
      "10 CNN layers ---\n",
      "11 CNN layers ---\n",
      "12 CNN layers ---\n",
      "13 CNN layers ---\n",
      "14 CNN layers ---\n",
      "15 CNN layers ---\n",
      "16 CNN layers ---\n",
      "17 CNN layers ---\n",
      "18 CNN layers ---\n",
      "19 CNN layers ---\n",
      "20 CNN layers ---\n",
      "1 CNN layers ---\n",
      "2 CNN layers ---\n",
      "3 CNN layers ---\n",
      "4 CNN layers ---\n",
      "5 CNN layers ---\n",
      "6 CNN layers ---\n",
      "7 CNN layers ---\n",
      "8 CNN layers ---\n",
      "9 CNN layers ---\n",
      "10 CNN layers ---\n",
      "11 CNN layers ---\n",
      "12 CNN layers ---\n",
      "13 CNN layers ---\n",
      "14 CNN layers ---\n",
      "15 CNN layers ---\n",
      "16 CNN layers ---\n",
      "17 CNN layers ---\n",
      "18 CNN layers ---\n",
      "19 CNN layers ---\n",
      "20 CNN layers ---\n"
     ]
    }
   ],
   "source": [
    "for experiment, one_layer in type_layer.items():\n",
    "    for num_layers in range(1, 21):  \n",
    "        print(f\"{num_layers} CNN layers ---\")\n",
    "\n",
    "        layers = []\n",
    "        in_channels = C\n",
    "        num_maxpool = 0  # Track the number of max pooling layers added\n",
    "        cnn_layer_count = 0\n",
    "\n",
    "        # Calculate the interval for adding max pooling layers\n",
    "        pooling_interval = max(1, num_layers // 3)  # Ensure at least 1\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            # Add a CNN layer\n",
    "            layers.append(one_layer(in_channels, n_filters))\n",
    "            in_channels = n_filters\n",
    "            cnn_layer_count += 1\n",
    "\n",
    "            # Add MaxPool2d if conditions are met\n",
    "            if num_layers > 3 and num_maxpool < 3 and cnn_layer_count >= pooling_interval:\n",
    "                layers.append(nn.MaxPool2d((2, 2)))\n",
    "                num_maxpool += 1\n",
    "                cnn_layer_count = 0 #reset counter\n",
    "\n",
    "        # Adaptive Pooling and Flatten\n",
    "        adaptive_size = 8\n",
    "        layers.append(nn.AdaptiveAvgPool2d((adaptive_size, adaptive_size)))\n",
    "        layers.append(nn.Flatten())\n",
    "\n",
    "        # Calculate the size of the input to the linear layer\n",
    "        flattened_size = n_filters * adaptive_size * adaptive_size\n",
    "        layers.append(nn.Linear(flattened_size, classes))\n",
    "\n",
    "        # Create the model\n",
    "        model = nn.Sequential(*layers)\n",
    "        with open(f'./model_summary/{experiment}_{num_layers}.txt', 'w') as f:\n",
    "            f.write(str(summary(model, input_size=(batch_size, C, W, H))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35894d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torchsummary import summary\n",
    "\n",
    "# --- Configuration (Example Values) ---\n",
    "C = 3  # Input channels (e.g., RGB)\n",
    "n_filters = 32  # Number of filters in the first layer\n",
    "classes = 10  # Number of output classes\n",
    "D = 28  # Input image dimension (assuming square images)\n",
    "W = D\n",
    "H = D\n",
    "batch_size = 32\n",
    "\n",
    "# --- Helper Function for CNN Layer ---\n",
    "def conv_layer(in_channels, out_channels=None):\n",
    "    if out_channels is None:\n",
    "        out_channels = in_channels\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "experiment = 'cnn'\n",
    "# --- Main Loop to Define and Summarize CNN Models ---\n",
    "if __name__ == '__main__':\n",
    "    for num_layers in range(1, 21):  # Generate models with 1 to 20 CNN layers\n",
    "        print(f\"\\n--- Model with {num_layers} CNN layers ---\")\n",
    "\n",
    "        layers = []\n",
    "        in_channels = C\n",
    "        num_maxpool = 0  # Track the number of max pooling layers added\n",
    "        cnn_since_pool = 0 #Track layers since last pool\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            # Add a CNN layer\n",
    "            layers.append(conv_layer(in_channels, n_filters))\n",
    "            in_channels = n_filters #next layer will use n_filters as input\n",
    "            cnn_since_pool += 1\n",
    "\n",
    "            # Add MaxPool2d if conditions are met\n",
    "            if num_layers > 3 and num_maxpool < 3 and cnn_since_pool >= 3:\n",
    "                layers.append(nn.MaxPool2d((2, 2)))\n",
    "                num_maxpool += 1\n",
    "                cnn_since_pool = 0 #reset counter\n",
    "\n",
    "        # Adaptive Pooling and Flatten\n",
    "        adaptive_size = 8\n",
    "        layers.append(nn.AdaptiveAvgPool2d((adaptive_size, adaptive_size)))\n",
    "        layers.append(nn.Flatten())\n",
    "\n",
    "        # Calculate the size of the input to the linear layer\n",
    "        flattened_size = n_filters * adaptive_size * adaptive_size\n",
    "        layers.append(nn.Linear(flattened_size, classes))\n",
    "\n",
    "        # Create the model\n",
    "        model = nn.Sequential(*layers)\n",
    "\n",
    "        # Print the model summary\n",
    "        try:\n",
    "            with open(f'./model_summary/{experiment}_{num_layers}.txt', 'w') as f:\n",
    "                f.write(str(summary(model, input_size=(batch_size, C, W, H))))        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during summary: {e}\")\n",
    "            continue  # Skip to the next model if there's an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0630c15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model with 1 CNN layers ---\n",
      "\n",
      "--- Model with 2 CNN layers ---\n",
      "\n",
      "--- Model with 3 CNN layers ---\n",
      "\n",
      "--- Model with 4 CNN layers ---\n",
      "\n",
      "--- Model with 5 CNN layers ---\n",
      "\n",
      "--- Model with 6 CNN layers ---\n",
      "\n",
      "--- Model with 7 CNN layers ---\n",
      "\n",
      "--- Model with 8 CNN layers ---\n",
      "\n",
      "--- Model with 9 CNN layers ---\n",
      "\n",
      "--- Model with 10 CNN layers ---\n",
      "\n",
      "--- Model with 11 CNN layers ---\n",
      "\n",
      "--- Model with 12 CNN layers ---\n",
      "\n",
      "--- Model with 13 CNN layers ---\n",
      "\n",
      "--- Model with 14 CNN layers ---\n",
      "\n",
      "--- Model with 15 CNN layers ---\n",
      "\n",
      "--- Model with 16 CNN layers ---\n",
      "\n",
      "--- Model with 17 CNN layers ---\n",
      "\n",
      "--- Model with 18 CNN layers ---\n",
      "\n",
      "--- Model with 19 CNN layers ---\n",
      "\n",
      "--- Model with 20 CNN layers ---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torchsummary import summary\n",
    "\n",
    "# --- Configuration (Example Values) ---\n",
    "C = 3  # Input channels (e.g., RGB)\n",
    "n_filters = 32  # Number of filters in the first layer\n",
    "classes = 10  # Number of output classes\n",
    "D = 28  # Input image dimension (assuming square images)\n",
    "W = D\n",
    "H = D\n",
    "batch_size = 32\n",
    "experiment = 'cnn'\n",
    "# --- Helper Function for CNN Layer ---\n",
    "def conv_layer(in_channels, out_channels=None):\n",
    "    if out_channels is None:\n",
    "        out_channels = in_channels\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "# --- Main Loop to Define and Summarize CNN Models ---\n",
    "if __name__ == '__main__':\n",
    "    for num_layers in range(1, 21):  # Generate models with 1 to 20 CNN layers\n",
    "        print(f\"\\n--- Model with {num_layers} CNN layers ---\")\n",
    "\n",
    "        layers = []\n",
    "        in_channels = C\n",
    "        num_maxpool = 0  # Track the number of max pooling layers added\n",
    "        cnn_layer_count = 0\n",
    "\n",
    "        # Calculate the interval for adding max pooling layers\n",
    "        pooling_interval = max(1, num_layers // 3)  # Ensure at least 1\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            # Add a CNN layer\n",
    "            layers.append(conv_layer(in_channels, n_filters))\n",
    "            in_channels = n_filters\n",
    "            cnn_layer_count += 1\n",
    "\n",
    "            # Add MaxPool2d if conditions are met\n",
    "            if num_layers > 3 and num_maxpool < 3 and cnn_layer_count >= pooling_interval:\n",
    "                layers.append(nn.MaxPool2d((2, 2)))\n",
    "                num_maxpool += 1\n",
    "                cnn_layer_count = 0 #reset counter\n",
    "\n",
    "        # Adaptive Pooling and Flatten\n",
    "        adaptive_size = 8\n",
    "        layers.append(nn.AdaptiveAvgPool2d((adaptive_size, adaptive_size)))\n",
    "        layers.append(nn.Flatten())\n",
    "\n",
    "        # Calculate the size of the input to the linear layer\n",
    "        flattened_size = n_filters * adaptive_size * adaptive_size\n",
    "        layers.append(nn.Linear(flattened_size, classes))\n",
    "\n",
    "        # Create the model\n",
    "        model = nn.Sequential(*layers)\n",
    "\n",
    "        # Print the model summary\n",
    "        try:\n",
    "            with open(f'./model_summary/{experiment}_{num_layers}.txt', 'w') as f:\n",
    "                f.write(str(summary(model, input_size=(batch_size, C, W, H))))        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during summary: {e}\")\n",
    "            continue  # Skip to the next model if there's an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b762fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for experiment, one_layer in type_layer.items():\n",
    "    print(experiment)\n",
    "    for num_layers in range(15, 20):\n",
    "        print(num_layers)\n",
    "        # if num_layers <= 3:\n",
    "        #     num_maxpool = 1\n",
    "        # else:\n",
    "        #     num_maxpool = 2\n",
    "        layers = [one_layer(C, n_filters)]\n",
    "        in_channels = n_filters\n",
    "        # out_channels = n_filters\n",
    "        for layer in range(1, num_layers):\n",
    "            if layer % 3:\n",
    "                layers.append(one_layer(in_channels))\n",
    "            else:\n",
    "                num_maxpool **= 2 \n",
    "                layers.append(nn.MaxPool2d((2,2)))\n",
    "                layers.append(one_layer(in_channels, 2*in_channels))\n",
    "                in_channels *= 2\n",
    "        adaptive_size = 8 # Choose an appropriate size (e.g., 8x8)\n",
    "        layers.append(nn.AdaptiveAvgPool2d((adaptive_size, adaptive_size)))\n",
    "        layers.extend([nn.Flatten(), nn.Linear(in_channels*adaptive_size*adaptive_size, classes)])\n",
    "        model = nn.Sequential(*layers)\n",
    "        # summary(model, input_size=(batch_size, C, W, H))\n",
    "        with open(f'./model_summary/{experiment}_{num_layers}.txt', 'w') as f:\n",
    "            f.write(str(summary(model, input_size=(batch_size, C, W, H))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3585b1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D * in_channels // num_maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a861185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "033350bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_maxpool = 2\n",
    "num_maxpool **= 2\n",
    "num_maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c5222ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1)\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1)\n",
       "  )\n",
       "  (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1)\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1)\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1)\n",
       "  )\n",
       "  (7): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (8): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1)\n",
       "  )\n",
       "  (9): Sequential(\n",
       "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1)\n",
       "  )\n",
       "  (10): Sequential(\n",
       "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1)\n",
       "  )\n",
       "  (11): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (12): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1)\n",
       "  )\n",
       "  (13): Flatten(start_dim=1, end_dim=-1)\n",
       "  (14): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c311200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7eff4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D *2* in_channels // num_maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9e8b2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D//(4*4*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a430956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Sequential                               --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Conv2d: 2-1                       320\n",
       "│    └─LeakyReLU: 2-2                    --\n",
       "├─Flatten: 1-2                           --\n",
       "├─Linear: 1-3                            125,450\n",
       "=================================================================\n",
       "Total params: 125,770\n",
       "Trainable params: 125,770\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d215f5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [256, 10]                 --\n",
       "├─Sequential: 1-1                        [256, 32, 28, 28]         --\n",
       "│    └─Conv2d: 2-1                       [256, 32, 28, 28]         320\n",
       "├─Sequential: 1-6                        --                        (recursive)\n",
       "│    └─LeakyReLU: 2-2                    [256, 32, 28, 28]         --\n",
       "├─Sequential: 1-3                        [256, 32, 28, 28]         --\n",
       "│    └─Conv2d: 2-3                       [256, 32, 28, 28]         9,248\n",
       "├─Sequential: 1-6                        --                        (recursive)\n",
       "│    └─LeakyReLU: 2-4                    [256, 32, 28, 28]         --\n",
       "├─Sequential: 1-5                        [256, 32, 28, 28]         --\n",
       "│    └─Conv2d: 2-5                       [256, 32, 28, 28]         9,248\n",
       "├─Sequential: 1-6                        --                        (recursive)\n",
       "│    └─LeakyReLU: 2-6                    [256, 32, 28, 28]         --\n",
       "├─MaxPool2d: 1-7                         [256, 32, 14, 14]         --\n",
       "├─Sequential: 1-8                        [256, 64, 14, 14]         --\n",
       "│    └─Conv2d: 2-7                       [256, 64, 14, 14]         18,496\n",
       "│    └─LeakyReLU: 2-8                    [256, 64, 14, 14]         --\n",
       "├─Flatten: 1-9                           [256, 12544]              --\n",
       "├─Linear: 1-10                           [256, 10]                 125,450\n",
       "==========================================================================================\n",
       "Total params: 162,762\n",
       "Trainable params: 162,762\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 4.74\n",
       "==========================================================================================\n",
       "Input size (MB): 0.80\n",
       "Forward/backward pass size (MB): 179.85\n",
       "Params size (MB): 0.65\n",
       "Estimated Total Size (MB): 181.31\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(batch_size, C, W, H))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
