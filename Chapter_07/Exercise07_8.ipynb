{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c8c2558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79042390",
   "metadata": {},
   "source": [
    "# Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039dd7ea",
   "metadata": {},
   "source": [
    "<img src=\"./images/08.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe87651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7285/676631470.py:10: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from utils import train_network, View, set_seed, weight_reset\n",
    "import mlflow\n",
    "from torchinfo import summary\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2306505",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI'] = './mlruns07_8'\n",
    "mlflow.set_tracking_uri(os.environ.get('MLFLOW_TRACKING_URI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a32380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/27 07:43:52 INFO mlflow.tracking.fluent: Experiment with name 'Exercise07_7' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_07/mlruns07_7/521635938366561066', creation_time=1750997632775, experiment_id='521635938366561066', last_update_time=1750997632775, lifecycle_stage='active', name='Exercise07_7', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment('Exercise07_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "878366d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e763208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf768bd",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7098bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "\n",
    "all_data = []\n",
    "resp = urlopen(\"https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt\")\n",
    "shakespear_100k = resp.read()\n",
    "shakespear_100k = shakespear_100k.decode('utf-8').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf06d88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size:  36\n",
      "Total Characters: 99993\n"
     ]
    }
   ],
   "source": [
    "vocab2indx = {} #the vocab $\\Sigma$\n",
    "for char in shakespear_100k: \n",
    "    if char not in vocab2indx: #add every new character to the vocab\n",
    "        vocab2indx[char] = len(vocab2indx) #set the index based on the current vocab size\n",
    "\n",
    "#Some useful code to goe from index back to original characters. \n",
    "indx2vocab = {}\n",
    "#Well simply iterate over all key,value pairs and create a dicionary with the inverse mapping. \n",
    "for k, v in vocab2indx.items():\n",
    "    indx2vocab[v] = k\n",
    "print(\"Vocab Size: \", len(vocab2indx))\n",
    "print(\"Total Characters:\", len(shakespear_100k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d13e54c",
   "metadata": {},
   "source": [
    "### Original Dataset: start at everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c69ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRegressiveDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates an autoregressive dataset from one single, long, source sequence by breaking it up into \"chunks\". \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, large_string, MAX_CHUNK=500):\n",
    "        \"\"\"\n",
    "        large_string: the original long source sequence that chunks will be extracted from\n",
    "        MAX_CHUNK: the maximum allowed size of any chunk. \n",
    "        \"\"\"\n",
    "        self.doc = large_string\n",
    "        self.MAX_CHUNK = MAX_CHUNK\n",
    "\n",
    "    def __len__(self):\n",
    "        #The number of items is the number of characters divided by chunk size\n",
    "        return (len(self.doc)-1) // self.MAX_CHUNK\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #Compute the starting position for the idx'th chunk\n",
    "        start = idx*self.MAX_CHUNK\n",
    "        #Grab the input sub-string\n",
    "        sub_string = self.doc[start:start+self.MAX_CHUNK]\n",
    "        #convert the sub-string into integers based on our vocab\n",
    "        x = [vocab2indx[c] for c in sub_string]\n",
    "        \n",
    "        #grab the label sub-string by shifting over by 1\n",
    "        sub_string = self.doc[start+1:start+self.MAX_CHUNK+1]\n",
    "        #convert the label sub-string into integers based on our vocab\n",
    "        y = [vocab2indx[c] for c in sub_string]\n",
    "        #convert the \n",
    "        return torch.tensor(x, dtype=torch.int64), torch.tensor(y, dtype=torch.int64)\n",
    "#Caption: Creating a dataset for autoregressive problems from a large text corpus. We assume the corpus exists as one long string, and it is OK to concatenate multiple files together into one long string since our chunks are smaller than most documents are anyway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84101c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoRegData_everywhere = AutoRegressiveDataset(shakespear_100k, MAX_CHUNK=250)\n",
    "batch_size = 128\n",
    "autoReg_loader_everywhere = DataLoader(autoRegData_everywhere, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3096a4a",
   "metadata": {},
   "source": [
    "### New Dataset: start at new lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179d51fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewLineAutoRegressiveDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates an autoregressive dataset from one single, long, source sequence by breaking it up into \"chunks\".\n",
    "    Each chunk will start at the beginning of a new line.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, large_string, MAX_CHUNK=500):\n",
    "        \"\"\"\n",
    "        large_string: the original long source sequence that chunks will be extracted from\n",
    "        MAX_CHUNK: the maximum allowed size of any chunk.\n",
    "        \"\"\"\n",
    "        self.doc = large_string\n",
    "        self.MAX_CHUNK = MAX_CHUNK\n",
    "\n",
    "        # Pre-filter newline indices to only include those that can form a valid chunk\n",
    "        self.valid_start_indices = []\n",
    "        if len(self.doc) >= self.MAX_CHUNK + 1 and self.doc[0] != '\\n':\n",
    "            self.valid_start_indices.append(0)\n",
    "        for i, char in enumerate(large_string):\n",
    "            if char == '\\n':\n",
    "                # +1 to start after the newline, +MAX_CHUNK for the full chunk length\n",
    "                # We need MAX_CHUNK + 1 for 'y' as well\n",
    "                if (i + 1 + MAX_CHUNK) <= len(self.doc): # Check if there's enough data for a full chunk\n",
    "                    self.valid_start_indices.append(i + 1)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_start_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_char_index = self.valid_start_indices[idx]\n",
    "\n",
    "        # Grab the input sub-string for x (length MAX_CHUNK)\n",
    "        sub_string_x = self.doc[start_char_index : start_char_index + self.MAX_CHUNK]\n",
    "        # Grab the input sub-string for y (shifted by 1, same length)\n",
    "        sub_string_y = self.doc[start_char_index + 1 : start_char_index + self.MAX_CHUNK + 1]\n",
    "\n",
    "        # Convert to integers based on our vocab\n",
    "        x = [vocab2indx[c] for c in sub_string_x]\n",
    "        y = [vocab2indx[c] for c in sub_string_y]\n",
    "\n",
    "        return torch.tensor(x, dtype=torch.int64), torch.tensor(y, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab51128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoRegData_newline = NewLineAutoRegressiveDataset(shakespear_100k, MAX_CHUNK=250)\n",
    "batch_size = 128\n",
    "autoReg_loader_newline = DataLoader(autoRegData_newline, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e000614",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRegressiveGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeddings, embd_size, hidden_size, layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embd = nn.Embedding(num_embeddings, embd_size)\n",
    "        self.layers = nn.ModuleList([nn.GRUCell(embd_size, hidden_size)] + \n",
    "                                    [nn.GRUCell(hidden_size, hidden_size) for i in range(layers-1)])\n",
    "        self.norms = nn.ModuleList([nn.LayerNorm(hidden_size) for i in range(layers)])\n",
    "        \n",
    "        self.pred_class = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),# (B, *, D)\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LayerNorm(hidden_size), # (B, *, D)\n",
    "            nn.Linear(hidden_size, num_embeddings) #(B, *. D) -> B(B, *, VocabSize)\n",
    "        )\n",
    "        \n",
    "    def initHiddenStates(self, B):\n",
    "        \"\"\"\n",
    "        Creates an initial hidden state list for the RNN layers. \n",
    "        \n",
    "        B: the batch size for the hidden states. \n",
    "        \"\"\"\n",
    "        return [torch.zeros(B, self.hidden_size, device=device) for _ in range(len(self.layers))]\n",
    "        \n",
    "    def step(self, x_in, h_prevs=None):\n",
    "        \"\"\"\n",
    "        x_in: the input for this current time step and has shape (B) if the values need \n",
    "            to be embedded, and (B, D) if they have alreayd been embedded. \n",
    "\n",
    "        h_prevs: a list of hidden state tensors each with shape (B, self.hidden_size) for each \n",
    "            layer in the network. These contain the current hidden state of the RNN layers and \n",
    "            will be updated by this call. \n",
    "        \"\"\"\n",
    "        #Prep all three arguments to be in the final form\n",
    "        if len(x_in.shape) == 1: #(B), we need to embed it\n",
    "            x_in = self.embd(x_in) #now (B, D)\n",
    "\n",
    "        if h_prevs is None:\n",
    "            h_prevs = self.initHiddenStates(x_in.shape[0])\n",
    "        \n",
    "        #Process the input \n",
    "        for l in range(len(self.layers)):\n",
    "            h_prev = h_prevs[l]\n",
    "            h = self.norms[l](self.layers[l](x_in, h_prev))\n",
    "\n",
    "            h_prevs[l] = h\n",
    "            x_in = h\n",
    "        #Make predictions about the token\n",
    "        return self.pred_class(x_in), h_prevs\n",
    "    \n",
    "    def forward(self, input):\n",
    "        #Input should be (B, T)\n",
    "        #What is the batch size?\n",
    "        B = input.size(0)\n",
    "        #What is the max number of time steps?\n",
    "        T = input.size(1)\n",
    "        \n",
    "        x = self.embd(input) #(B, T, D)\n",
    "        \n",
    "        #Initial hidden states\n",
    "        h_prevs = self.initHiddenStates(B)\n",
    "        \n",
    "        last_activations = []\n",
    "        for t in range(T):\n",
    "            x_in = x[:,t,:] #(B, D)\n",
    "            preds, h_prevs = self.step(x_in, h_prevs)\n",
    "            last_activations.append(preds)\n",
    "        \n",
    "        last_activations = torch.stack(last_activations, dim=1) #(B, T, D)\n",
    "        \n",
    "        return last_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6eac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoRegressiveGRU(len(vocab2indx), 32, 128, layers=2).to(device)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f954f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bd39d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntLossTime(x, y):\n",
    "    \"\"\"\n",
    "    x: output with shape (B, T, V)\n",
    "    y: labels with shape (B, T)\n",
    "    \n",
    "    \"\"\"\n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    \n",
    "    T = x.size(1)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(T):#for every item in the sequence\n",
    "        loss += cel(x[:,t,:], y[:,t]) #Compute the sum of prediction errors\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc9ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = CrossEntLossTime\n",
    "epochs = 5\n",
    "params = {\n",
    "    'device': device,\n",
    "    'loss_func': loss_func.__class__.__name__,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d1a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'start_at_everywhere': autoReg_loader_everywhere,\n",
    "    'start_at_newlines': autoReg_loader_newline,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df47f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [00:33<00:00, 33.98s/it]\n"
     ]
    }
   ],
   "source": [
    "for experiment, dataloader in dataloaders.items():\n",
    "    print(experiment)\n",
    "    model.apply(weight_reset)\n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "    with open('model_summary.txt', 'w') as f:\n",
    "        f.write(str(summary(model, inpt_size=(batch_size, 250, 1))))\n",
    "    with mlflow.start_run(nested=True, run_name=experiment):\n",
    "        params['optimizer'] = optimizer.defaults\n",
    "        mlflow.log_artifact('model_summary.txt')\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        results = train_network(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            loss_func=loss_func,\n",
    "            train_loader=dataloader,\n",
    "            epochs=epochs,\n",
    "            device=device,                \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e45c9",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cba63b",
   "metadata": {},
   "source": [
    "Yes, I absolutely think this change would significantly alter the characteristics of the generated output. Here's why:\n",
    "\n",
    "Improved Coherence at the Start of Generations:\n",
    "\n",
    "Original Dataset: Because the original dataset could grab a subsequence from anywhere in the text, the model would be trained on inputs that started mid-word or mid-sentence. When generating text, the model might produce outputs that feel abruptly cut off or grammatically incomplete at the beginning of its generations, as it learned to complete fragments.\n",
    "New Dataset: By forcing each training sequence to begin at the start of a new line, the model will learn to generate text that naturally flows from a \"fresh\" start. This is crucial for tasks like generating paragraphs, dialogue, or code snippets where new lines often signify new ideas or complete statements. The generated output is likely to be more grammatically sound and contextually appropriate from its very first characters.\n",
    "Learning Line-Level Structure and Pacing:\n",
    "\n",
    "Original Dataset: The original dataset didn't explicitly reinforce the concept of line breaks. The model might have learned some implicit patterns related to line breaks, but it wasn't a primary signal.\n",
    "New Dataset: The new dataset emphasizes the importance of new lines. The model will be exposed to patterns of how sentences and phrases begin after a line break. This could lead to generated text that better respects line formatting, pacing, and the typical structure of written content (e.g., poetry, dialogue, prose where new lines indicate a new speaker or thought).\n",
    "Potential Impact on Vocabulary Usage at Start of Lines:\n",
    "\n",
    "Models often learn associations between starting words/characters and the context that follows. By consistently starting after a newline, the model might develop a stronger understanding of the typical words or phrases that begin a new line in the training data (e.g., common sentence starters, names in dialogue). This could subtly influence the vocabulary and phrasing at the beginning of generated sequences.\n",
    "Slightly Different Statistical Distribution of Input:\n",
    "\n",
    "Even though the overall character distribution remains the same, the sequences the model sees are now structurally different. The model will no longer see sequences like \"ing the \" or \"s of th\", but rather sequences that consistently start with the first character of a line (e.g., \"The\", \"And\", \"Enter\", a speaker's name, etc.). This change in the statistical distribution of the input data will inevitably affect the learned parameters of the model and, consequently, the generated output.\n",
    "In summary, the NewAutoRegressiveDataset encourages the model to learn more about the structure and conventions of text at the line level. This should result in generated text that is more coherent, natural-sounding, and adheres better to typical textual formatting, especially at the beginning of generated passages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
