{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c8c2558",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79042390",
   "metadata": {},
   "source": [
    "# Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039dd7ea",
   "metadata": {},
   "source": [
    "<img src=\"./images/07.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe87651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3308/676631470.py:10: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from utils import train_network, View, set_seed\n",
    "import mlflow\n",
    "from torchinfo import summary\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2306505",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI'] = './mlruns07_7'\n",
    "mlflow.set_tracking_uri(os.environ.get('MLFLOW_TRACKING_URI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9a32380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/27 07:43:52 INFO mlflow.tracking.fluent: Experiment with name 'Exercise07_7' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_07/mlruns07_7/521635938366561066', creation_time=1750997632775, experiment_id='521635938366561066', last_update_time=1750997632775, lifecycle_stage='active', name='Exercise07_7', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment('Exercise07_7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "878366d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e763208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf768bd",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7098bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "\n",
    "all_data = []\n",
    "resp = urlopen(\"https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt\")\n",
    "shakespear_100k = resp.read()\n",
    "shakespear_100k = shakespear_100k.decode('utf-8').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf06d88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size:  36\n",
      "Total Characters: 99993\n"
     ]
    }
   ],
   "source": [
    "vocab2indx = {} #the vocab $\\Sigma$\n",
    "for char in shakespear_100k: \n",
    "    if char not in vocab2indx: #add every new character to the vocab\n",
    "        vocab2indx[char] = len(vocab2indx) #set the index based on the current vocab size\n",
    "\n",
    "#Some useful code to goe from index back to original characters. \n",
    "indx2vocab = {}\n",
    "#Well simply iterate over all key,value pairs and create a dicionary with the inverse mapping. \n",
    "for k, v in vocab2indx.items():\n",
    "    indx2vocab[v] = k\n",
    "print(\"Vocab Size: \", len(vocab2indx))\n",
    "print(\"Total Characters:\", len(shakespear_100k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40ee7e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRegressiveDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates an autoregressive dataset from one single, long, source sequence by breaking it up into \"chunks\". \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, large_string, MAX_CHUNK=500):\n",
    "        \"\"\"\n",
    "        large_string: the original long source sequence that chunks will be extracted from\n",
    "        MAX_CHUNK: the maximum allowed size of any chunk. \n",
    "        \"\"\"\n",
    "        self.doc = large_string\n",
    "        self.MAX_CHUNK = MAX_CHUNK\n",
    "\n",
    "    def __len__(self):\n",
    "        #The number of items is the number of characters divided by chunk size\n",
    "        return (len(self.doc)-1) // self.MAX_CHUNK\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #Compute the starting position for the idx'th chunk\n",
    "        start = idx*self.MAX_CHUNK\n",
    "        #Grab the input sub-string\n",
    "        sub_string = self.doc[start:start+self.MAX_CHUNK]\n",
    "        #convert the sub-string into integers based on our vocab\n",
    "        x = [vocab2indx[c] for c in sub_string]\n",
    "        \n",
    "        #grab the label sub-string by shifting over by 1\n",
    "        sub_string = self.doc[start+1:start+self.MAX_CHUNK+1]\n",
    "        #convert the label sub-string into integers based on our vocab\n",
    "        y = [vocab2indx[c] for c in sub_string]\n",
    "        #convert the \n",
    "        return torch.tensor(x, dtype=torch.int64), torch.tensor(y, dtype=torch.int64)\n",
    "#Caption: Creating a dataset for autoregressive problems from a large text corpus. We assume the corpus exists as one long string, and it is OK to concatenate multiple files together into one long string since our chunks are smaller than most documents are anyway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c0f2882",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoRegData = AutoRegressiveDataset(shakespear_100k, MAX_CHUNK=250)\n",
    "batch_size = 128\n",
    "autoReg_loader = DataLoader(autoRegData, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e000614",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608fc006",
   "metadata": {},
   "source": [
    "### RGUcell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRegressiveGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeddings, embd_size, hidden_size, layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embd = nn.Embedding(num_embeddings, embd_size)\n",
    "        self.layers = nn.ModuleList([nn.GRUCell(embd_size, hidden_size)] + \n",
    "                                    [nn.GRUCell(hidden_size, hidden_size) for i in range(layers-1)])\n",
    "        self.norms = nn.ModuleList([nn.LayerNorm(hidden_size) for i in range(layers)])\n",
    "        \n",
    "        self.pred_class = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),# (B, *, D)\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LayerNorm(hidden_size), # (B, *, D)\n",
    "            nn.Linear(hidden_size, num_embeddings) #(B, *. D) -> B(B, *, VocabSize)\n",
    "        )\n",
    "        \n",
    "    def initHiddenStates(self, B):\n",
    "        \"\"\"\n",
    "        Creates an initial hidden state list for the RNN layers. \n",
    "        \n",
    "        B: the batch size for the hidden states. \n",
    "        \"\"\"\n",
    "        return [torch.zeros(B, self.hidden_size, device=device) for _ in range(len(self.layers))]\n",
    "        \n",
    "    def step(self, x_in, h_prevs=None):\n",
    "        \"\"\"\n",
    "        x_in: the input for this current time step and has shape (B) if the values need \n",
    "            to be embedded, and (B, D) if they have alreayd been embedded. \n",
    "\n",
    "        h_prevs: a list of hidden state tensors each with shape (B, self.hidden_size) for each \n",
    "            layer in the network. These contain the current hidden state of the RNN layers and \n",
    "            will be updated by this call. \n",
    "        \"\"\"\n",
    "        #Prep all three arguments to be in the final form\n",
    "        if len(x_in.shape) == 1: #(B), we need to embed it\n",
    "            x_in = self.embd(x_in) #now (B, D)\n",
    "\n",
    "        if h_prevs is None:\n",
    "            h_prevs = self.initHiddenStates(x_in.shape[0])\n",
    "        \n",
    "        #Process the input \n",
    "        for l in range(len(self.layers)):\n",
    "            h_prev = h_prevs[l]\n",
    "            h = self.norms[l](self.layers[l](x_in, h_prev))\n",
    "\n",
    "            h_prevs[l] = h\n",
    "            x_in = h\n",
    "        #Make predictions about the token\n",
    "        return self.pred_class(x_in), h_prevs\n",
    "    \n",
    "    def forward(self, input):\n",
    "        #Input should be (B, T)\n",
    "        #What is the batch size?\n",
    "        B = input.size(0)\n",
    "        #What is the max number of time steps?\n",
    "        T = input.size(1)\n",
    "        \n",
    "        x = self.embd(input) #(B, T, D)\n",
    "        \n",
    "        #Initial hidden states\n",
    "        h_prevs = self.initHiddenStates(B)\n",
    "        \n",
    "        last_activations = []\n",
    "        for t in range(T):\n",
    "            x_in = x[:,t,:] #(B, D)\n",
    "            pred, h_prevs = self.step(x_in, h_prevs)\n",
    "            last_activations.append(pred)\n",
    "        \n",
    "        last_activations = torch.stack(last_activations, dim=1) #(B, T, D)\n",
    "        \n",
    "        return last_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6eac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoReg_model_gru = AutoRegressiveGRU(len(vocab2indx), 32, 128, layers=2)\n",
    "autoReg_model_gru = autoReg_model_gru.to(device)\n",
    "\n",
    "for p in autoReg_model_gru.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104a619e",
   "metadata": {},
   "source": [
    "### LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9d1628",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRegressiveLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeddings, embd_size, hidden_size, layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embd = nn.Embedding(num_embeddings, embd_size)\n",
    "        self.layers = nn.ModuleList([nn.LSTMCell(embd_size, hidden_size)] + \n",
    "                                    [nn.LSTMCell(hidden_size, hidden_size) for i in range(layers-1)])\n",
    "        self.norms = nn.ModuleList([nn.LayerNorm(hidden_size) for i in range(layers)])\n",
    "        \n",
    "        self.pred_class = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),# (B, *, D)\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LayerNorm(hidden_size), # (B, *, D)\n",
    "            nn.Linear(hidden_size, num_embeddings) #(B, *. D) -> B(B, *, VocabSize)\n",
    "        )\n",
    "        \n",
    "    def initHiddenStates(self, B):\n",
    "        \"\"\"\n",
    "        Creates an initial hidden state list for the RNN layers. \n",
    "        \n",
    "        B: the batch size for the hidden states. \n",
    "        \"\"\"\n",
    "        return [torch.zeros(B, self.hidden_size, device=device) for _ in range(len(self.layers))]\n",
    "        \n",
    "    def step(self, x_in, h_prevs=None, c_prevs=None):\n",
    "        \"\"\"\n",
    "        x_in: the input for this current time step and has shape (B) if the values need \n",
    "            to be embedded, and (B, D) if they have alreayd been embedded. \n",
    "\n",
    "        h_prevs: a list of hidden state tensors each with shape (B, self.hidden_size) for each \n",
    "            layer in the network. These contain the current hidden state of the RNN layers and \n",
    "            will be updated by this call. \n",
    "        \"\"\"\n",
    "        #Prep all three arguments to be in the final form\n",
    "        if len(x_in.shape) == 1: #(B), we need to embed it\n",
    "            x_in = self.embd(x_in) #now (B, D)\n",
    "\n",
    "        if h_prevs is None:\n",
    "            h_prevs = self.initHiddenStates(x_in.shape[0])\n",
    "        if c_prevs is None:\n",
    "            c_prevs = self.initHiddenStates(x_in.shape[0])\n",
    "        #Process the input \n",
    "        for l in range(len(self.layers)):\n",
    "            h_prev = h_prevs[l]\n",
    "            c_prev = c_prevs[l]\n",
    "            h, c = self.layers[l](x_in, (h_prev, c_prev))\n",
    "            h = self.norms[l](h)\n",
    "\n",
    "            h_prevs[l] = h\n",
    "            c_prevs[l] = c\n",
    "            x_in = h\n",
    "        #Make predictions about the token\n",
    "        return self.pred_class(x_in), h_prevs, c_prevs\n",
    "    \n",
    "    def forward(self, input):\n",
    "        #Input should be (B, T)\n",
    "        #What is the batch size?\n",
    "        B = input.size(0)\n",
    "        #What is the max number of time steps?\n",
    "        T = input.size(1)\n",
    "        \n",
    "        x = self.embd(input) #(B, T, D)\n",
    "        \n",
    "        #Initial hidden states\n",
    "        h_prevs = self.initHiddenStates(B)\n",
    "        c_prevs = self.initHiddenStates(B)\n",
    "        \n",
    "        last_activations = []\n",
    "        for t in range(T):\n",
    "            x_in = x[:,t,:] #(B, D)\n",
    "            pred, h_prevs, c_prevs = self.step(x_in, h_prevs, c_prevs)\n",
    "            last_activations.append(pred)\n",
    "        \n",
    "        last_activations = torch.stack(last_activations, dim=1) #(B, T, D)\n",
    "        \n",
    "        return last_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoReg_model_lstm = AutoRegressiveLSTM(len(vocab2indx), 32, 128, layers=2)\n",
    "autoReg_model_lstm = autoReg_model_lstm.to(device)\n",
    "\n",
    "for p in autoReg_model_lstm.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f954f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bd39d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntLossTime(x, y):\n",
    "    \"\"\"\n",
    "    x: output with shape (B, T, V)\n",
    "    y: labels with shape (B, T)\n",
    "    \n",
    "    \"\"\"\n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    \n",
    "    T = x.size(1)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(T):#for every item in the sequence\n",
    "        loss += cel(x[:,t,:], y[:,t]) #Compute the sum of prediction errors\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc9ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = CrossEntLossTime\n",
    "epochs = 5\n",
    "params = {\n",
    "    'device': device,\n",
    "    'loss_func': loss_func.__class__.__name__,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7890ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'GRU': autoReg_model_gru,\n",
    "    'LSTM': autoReg_model_lstm,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df47f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [00:33<00:00, 33.98s/it]\n"
     ]
    }
   ],
   "source": [
    "for experiment, model in models.items():\n",
    "    print(experiment)\n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "    with open('model_summary.txt', 'w') as f:\n",
    "        f.write(str(summary(model, inpt_size=(batch_size, 250, 1))))\n",
    "    with mlflow.start_run(nested=True, run_name=experiment):\n",
    "        params['optimizer'] = optimizer.defaults\n",
    "        mlflow.log_artifact('model_summary.txt')\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        results = train_network(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            loss_func=loss_func,\n",
    "            train_loader=autoReg_loader,\n",
    "            epochs=epochs,\n",
    "            device=device,                \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4b7c47",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e45c9",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f2e90e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
