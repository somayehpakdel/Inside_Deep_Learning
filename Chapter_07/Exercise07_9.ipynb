{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c8c2558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79042390",
   "metadata": {},
   "source": [
    "# Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039dd7ea",
   "metadata": {},
   "source": [
    "<img src=\"./images/09.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe87651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7285/676631470.py:10: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from utils import train_network, View, set_seed\n",
    "import mlflow\n",
    "from torchinfo import summary\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2306505",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI'] = './mlruns07_8'\n",
    "mlflow.set_tracking_uri(os.environ.get('MLFLOW_TRACKING_URI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a32380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/27 07:43:52 INFO mlflow.tracking.fluent: Experiment with name 'Exercise07_7' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_07/mlruns07_7/521635938366561066', creation_time=1750997632775, experiment_id='521635938366561066', last_update_time=1750997632775, lifecycle_stage='active', name='Exercise07_7', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment('Exercise07_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "878366d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e763208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf768bd",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7098bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "\n",
    "all_data = []\n",
    "resp = urlopen(\"https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt\")\n",
    "shakespear_100k = resp.read()\n",
    "shakespear_100k = shakespear_100k.decode('utf-8').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf06d88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size:  36\n",
      "Total Characters: 99993\n"
     ]
    }
   ],
   "source": [
    "vocab2indx = {} #the vocab $\\Sigma$\n",
    "for char in shakespear_100k: \n",
    "    if char not in vocab2indx: #add every new character to the vocab\n",
    "        vocab2indx[char] = len(vocab2indx) #set the index based on the current vocab size\n",
    "\n",
    "#Some useful code to goe from index back to original characters. \n",
    "indx2vocab = {}\n",
    "#Well simply iterate over all key,value pairs and create a dicionary with the inverse mapping. \n",
    "for k, v in vocab2indx.items():\n",
    "    indx2vocab[v] = k\n",
    "print(\"Vocab Size: \", len(vocab2indx))\n",
    "print(\"Total Characters:\", len(shakespear_100k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3096a4a",
   "metadata": {},
   "source": [
    "### New Dataset: start at new lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179d51fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewLineAutoRegressiveDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates an autoregressive dataset from one single, long, source sequence by breaking it up into \"chunks\".\n",
    "    Each chunk will start at the beginning of a new line.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, large_string, MAX_CHUNK=500):\n",
    "        \"\"\"\n",
    "        large_string: the original long source sequence that chunks will be extracted from\n",
    "        MAX_CHUNK: the maximum allowed size of any chunk.\n",
    "        \"\"\"\n",
    "        self.doc = large_string\n",
    "        self.MAX_CHUNK = MAX_CHUNK\n",
    "\n",
    "        # Pre-filter newline indices to only include those that can form a valid chunk\n",
    "        self.valid_start_indices = []\n",
    "        if len(self.doc) >= self.MAX_CHUNK + 1 and self.doc[0] != '\\n':\n",
    "            self.valid_start_indices.append(0)\n",
    "        for i, char in enumerate(large_string):\n",
    "            if char == '\\n':\n",
    "                # +1 to start after the newline, +MAX_CHUNK for the full chunk length\n",
    "                # We need MAX_CHUNK + 1 for 'y' as well\n",
    "                if (i + 1 + MAX_CHUNK) <= len(self.doc): # Check if there's enough data for a full chunk\n",
    "                    self.valid_start_indices.append(i + 1)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_start_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_char_index = self.valid_start_indices[idx]\n",
    "\n",
    "        # Grab the input sub-string for x (length MAX_CHUNK)\n",
    "        sub_string_x = self.doc[start_char_index : start_char_index + self.MAX_CHUNK]\n",
    "        # Grab the input sub-string for y (shifted by 1, same length)\n",
    "        sub_string_y = self.doc[start_char_index + 1 : start_char_index + self.MAX_CHUNK + 1]\n",
    "\n",
    "        # Convert to integers based on our vocab\n",
    "        x = [vocab2indx[c] for c in sub_string_x]\n",
    "        y = [vocab2indx[c] for c in sub_string_y]\n",
    "\n",
    "        return torch.tensor(x, dtype=torch.int64), torch.tensor(y, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab51128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoRegData_newline = NewLineAutoRegressiveDataset(shakespear_100k, MAX_CHUNK=250)\n",
    "batch_size = 128\n",
    "autoReg_loader_newline = DataLoader(autoRegData_newline, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e000614",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRegressiveGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeddings, embd_size, hidden_size, layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embd = nn.Embedding(num_embeddings, embd_size)\n",
    "        self.layers = nn.ModuleList([nn.GRUCell(embd_size, hidden_size)] + \n",
    "                                    [nn.GRUCell(hidden_size, hidden_size) for i in range(layers-1)])\n",
    "        self.norms = nn.ModuleList([nn.LayerNorm(hidden_size) for i in range(layers)])\n",
    "        \n",
    "        self.pred_class = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),# (B, *, D)\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LayerNorm(hidden_size), # (B, *, D)\n",
    "            nn.Linear(hidden_size, num_embeddings) #(B, *. D) -> B(B, *, VocabSize)\n",
    "        )\n",
    "        \n",
    "    def initHiddenStates(self, B):\n",
    "        \"\"\"\n",
    "        Creates an initial hidden state list for the RNN layers. \n",
    "        \n",
    "        B: the batch size for the hidden states. \n",
    "        \"\"\"\n",
    "        return [torch.zeros(B, self.hidden_size, device=device) for _ in range(len(self.layers))]\n",
    "        \n",
    "    def step(self, x_in, h_prevs=None):\n",
    "        \"\"\"\n",
    "        x_in: the input for this current time step and has shape (B) if the values need \n",
    "            to be embedded, and (B, D) if they have alreayd been embedded. \n",
    "\n",
    "        h_prevs: a list of hidden state tensors each with shape (B, self.hidden_size) for each \n",
    "            layer in the network. These contain the current hidden state of the RNN layers and \n",
    "            will be updated by this call. \n",
    "        \"\"\"\n",
    "        #Prep all three arguments to be in the final form\n",
    "        if len(x_in.shape) == 1: #(B), we need to embed it\n",
    "            x_in = self.embd(x_in) #now (B, D)\n",
    "\n",
    "        if h_prevs is None:\n",
    "            h_prevs = self.initHiddenStates(x_in.shape[0])\n",
    "        \n",
    "        #Process the input \n",
    "        for l in range(len(self.layers)):\n",
    "            h_prev = h_prevs[l]\n",
    "            h = self.norms[l](self.layers[l](x_in, h_prev))\n",
    "\n",
    "            h_prevs[l] = h\n",
    "            x_in = h\n",
    "        #Make predictions about the token\n",
    "        return self.pred_class(x_in), h_prevs\n",
    "    \n",
    "    def forward(self, input):\n",
    "        #Input should be (B, T)\n",
    "        #What is the batch size?\n",
    "        B = input.size(0)\n",
    "        #What is the max number of time steps?\n",
    "        T = input.size(1)\n",
    "        \n",
    "        x = self.embd(input) #(B, T, D)\n",
    "        \n",
    "        #Initial hidden states\n",
    "        h_prevs = self.initHiddenStates(B)\n",
    "        \n",
    "        last_activations = []\n",
    "        for t in range(T):\n",
    "            x_in = x[:,t,:] #(B, D)\n",
    "            preds, h_prevs = self.step(x_in, h_prevs)\n",
    "            last_activations.append(preds)\n",
    "        \n",
    "        last_activations = torch.stack(last_activations, dim=1) #(B, T, D)\n",
    "        \n",
    "        return last_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6eac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoRegressiveGRU(len(vocab2indx), 32, 128, layers=2).to(device)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f954f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bd39d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntLossTime(x, y):\n",
    "    \"\"\"\n",
    "    x: output with shape (B, T, V)\n",
    "    y: labels with shape (B, T)\n",
    "    \n",
    "    \"\"\"\n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    \n",
    "    T = x.size(1)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(T):#for every item in the sequence\n",
    "        loss += cel(x[:,t,:], y[:,t]) #Compute the sum of prediction errors\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc9ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = CrossEntLossTime\n",
    "epochs = 5\n",
    "params = {\n",
    "    'device': device,\n",
    "    'loss_func': loss_func.__class__.__name__,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df47f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [00:33<00:00, 33.98s/it]\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(model.parameters())\n",
    "with open('model_summary.txt', 'w') as f:\n",
    "    f.write(str(summary(model, inpt_size=(batch_size, 250, 1))))\n",
    "with mlflow.start_run(nested=True, run_name='exercise_9'):\n",
    "    params['optimizer'] = optimizer.defaults\n",
    "    mlflow.log_artifact('model_summary.txt')\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    results = train_network(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss_func=loss_func,\n",
    "        train_loader=autoReg_loader_newline,\n",
    "        epochs=epochs,\n",
    "        device=device,                \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6917e82",
   "metadata": {},
   "source": [
    "## Feature Extracting with LastTimeStep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b9a606",
   "metadata": {},
   "source": [
    "LastTimeStep class from chapter 04:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastTimeStep(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for extracting the hidden activations of the last time step following \n",
    "    the output of a PyTorch RNN module. \n",
    "    \"\"\"\n",
    "    def __init__(self, rnn_layers=1, bidirectional=False):\n",
    "        super(LastTimeStep, self).__init__()\n",
    "        self.rnn_layers = rnn_layers\n",
    "        if bidirectional:\n",
    "            self.num_driections = 2\n",
    "        else:\n",
    "            self.num_driections = 1    \n",
    "    \n",
    "    def forward(self, input):\n",
    "        #Result is either a tupe (out, h_t)\n",
    "        #or a tuple (out, (h_t, c_t))\n",
    "        rnn_output = input[0]\n",
    "        last_step = input[1] #this will be h_t\n",
    "        if(type(last_step) == tuple):#unless it's a tuple, \n",
    "            last_step = last_step[0]#then h_t is the first item in the tuple\n",
    "        batch_size = last_step.shape[1] #per docs, shape is: '(num_layers * num_directions, batch, hidden_size)'\n",
    "        #reshaping so that everything is separate \n",
    "        last_step = last_step.view(self.rnn_layers, self.num_driections, batch_size, -1)\n",
    "        #We want the last layer's results\n",
    "        last_step = last_step[self.rnn_layers-1] \n",
    "        #Re order so batch comes first\n",
    "        last_step = last_step.permute(1, 0, 2)\n",
    "        #Finally, flatten the last two dimensions into one\n",
    "        return last_step.reshape(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9470e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_timestep_extractor = LastTimeStep(rnn_layers=2, bidirectional=False).to(device)\n",
    "all_sentence_features = []\n",
    "with torch.no_grad():\n",
    "    for input, label in tqdm(autoReg_loader_newline):\n",
    "        input.to(device)\n",
    "        predictions = model(input)\n",
    "        stacked_final_hidden_states = torch.stack(model.h_prevs, dim=0)\n",
    "        sentence_features_batch = last_timestep_extractor((predictions, stacked_final_hidden_states))\n",
    "\n",
    "        all_sentence_features.append(sentence_features_batch.cpu())\n",
    "final_feature_tensor = torch.cat(all_sentence_features, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1fff24",
   "metadata": {},
   "source": [
    "## Clustring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef7a67",
   "metadata": {},
   "source": [
    "\"Note: You may want to sub-sample a smaller number of sentences to make your clustering algorithm run faster.\"\n",
    "\n",
    "Clustering algorithms, especially on high-dimensional data (your hidden_size could be 128 or more), can be computationally expensive.\n",
    "If you have tens of thousands or hundreds of thousands of sentences, running clustering on all of them might take a very long time or consume too much memory.\n",
    "Sub-sampling: Randomly select a subset of your sentences (e.g., 5,000 or 10,000) to perform the clustering on. The results from this subset should still give you a good indication of the overall clusters present in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30e5bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "X = \n",
    "kmeans = KMeans(n_clusters=X)\n",
    "kmeans.fit(final_feature_tensor)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e45c9",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3469a5a",
   "metadata": {},
   "source": [
    "Why does this work? (Intuition)\n",
    "The hidden states of an RNN (like a GRU) are often thought of as a \"memory\" of the sequence processed so far. By the time the model has processed an entire sentence, its final hidden state summarizes the information it has learned from that sentence. If the model is good at predicting the next character, it implies that its hidden states are rich and discriminative. Sentences that are \"similar\" in content, style, or structure will likely lead to similar final hidden states, which is what clustering algorithms look for. This process is a common way to derive \"sentence embeddings\" for various NLP tasks.\n",
    "\n",
    "The overall goal is to use the learned internal representations of a generative model to perform an analytical task (clustering) to gain insights into the structure and patterns within the text data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
