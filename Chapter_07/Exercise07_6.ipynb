{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c8c2558",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79042390",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039dd7ea",
   "metadata": {},
   "source": [
    "<img src=\"./images/06.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe87651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50291/676631470.py:10: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from utils import train_network, View, set_seed\n",
    "import mlflow\n",
    "from torchinfo import summary\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2306505",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI'] = './mlruns07_6'\n",
    "mlflow.set_tracking_uri(os.environ.get('MLFLOW_TRACKING_URI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9a32380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_07/mlruns07_6/460950000382384422', creation_time=1750948549809, experiment_id='460950000382384422', last_update_time=1750948549809, lifecycle_stage='active', name='Exercise07_6', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment('Exercise07_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "878366d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e763208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf768bd",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dc3eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencodDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, index):\n",
    "        # x, y = self.dataset.__getitem__(index)\n",
    "        x, _ = self.dataset[index]\n",
    "        x = x.flatten().unsqueeze(-1)\n",
    "        return  x, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "920a2c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = AutoencodDataset(torchvision.datasets.MNIST(\"./data\", train=True, transform=transforms.ToTensor(), download=True))\n",
    "test_data_xy = torchvision.datasets.MNIST(\"./data\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_data_xx = AutoencodDataset(test_data_xy)\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data_xx, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e000614",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21a6870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 28\n",
    "H = 28\n",
    "D = W * H\n",
    "C =1\n",
    "classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae26bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRegressive(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = nn.ModuleList([nn.GRUCell(input_size, hidden_size)] + \n",
    "                                    [nn.GRUCell(hidden_size, hidden_size) for i in range(layers-1)])\n",
    "        self.norms = nn.ModuleList([nn.LayerNorm(hidden_size) for i in range(layers)])\n",
    "        \n",
    "        self.pred_class = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),# (B, *, D)\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LayerNorm(hidden_size), # (B, *, D)\n",
    "            nn.Linear(hidden_size, input_size) #(B, *. D) -> B(B, *, VocabSize)\n",
    "        )\n",
    "        \n",
    "    def initHiddenStates(self, B):\n",
    "        \"\"\"\n",
    "        Creates an initial hidden state list for the RNN layers. \n",
    "        \n",
    "        B: the batch size for the hidden states. \n",
    "        \"\"\"\n",
    "        return [torch.zeros(B, self.hidden_size, device=device) for _ in range(len(self.layers))]\n",
    "        \n",
    "    def step(self, x_in, h_prevs=None):\n",
    "        \"\"\"\n",
    "        x_in: the input for this current time step and has shape (B) if the values need \n",
    "            to be embedded, and (B, D) if they have alreayd been embedded. \n",
    "\n",
    "        h_prevs: a list of hidden state tensors each with shape (B, self.hidden_size) for each \n",
    "            layer in the network. These contain the current hidden state of the RNN layers and \n",
    "            will be updated by this call. \n",
    "        \"\"\"\n",
    "\n",
    "        if h_prevs is None:\n",
    "            h_prevs = self.initHiddenStates(x_in.shape[0])\n",
    "        \n",
    "        #Process the input \n",
    "        for l in range(len(self.layers)):\n",
    "            h_prev = h_prevs[l]\n",
    "            h = self.norms[l](self.layers[l](x_in, h_prev))\n",
    "\n",
    "            h_prevs[l] = h\n",
    "            x_in = h\n",
    "        #Make predictions about the token\n",
    "        return self.pred_class(x_in)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        #Input should be (B, T)\n",
    "        #What is the batch size?\n",
    "        B = input.size(0)\n",
    "        #What is the max number of time steps?\n",
    "        T = input.size(1)\n",
    "        \n",
    "        x = input\n",
    "        \n",
    "        #Initial hidden states\n",
    "        h_prevs = self.initHiddenStates(B)\n",
    "        \n",
    "        last_activations = []\n",
    "        for t in range(T):\n",
    "            x_in = x[:,t,:] #(B, D)\n",
    "            last_activations.append(self.step(x_in, h_prevs))\n",
    "        \n",
    "        last_activations = torch.stack(last_activations, dim=1) #(B, T, D)\n",
    "        \n",
    "        return last_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aa43e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoRegressive(1, 128, layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f954f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc9ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()\n",
    "epochs = 5\n",
    "params = {\n",
    "    'device': device,\n",
    "    'loss_func': loss_func.__class__.__name__,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df47f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(model.parameters())\n",
    "with open('model_summary.txt', 'w') as f:\n",
    "    f.write(str(summary(model, inpt_size=(batch_size, D))))\n",
    "with mlflow.start_run(nested=True, run_name='exercise_6'):\n",
    "    params['optimizer'] = optimizer.defaults\n",
    "    mlflow.log_artifact('model_summary.txt')\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    results = train_network(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss_func=loss_func,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=test_loader,\n",
    "        epochs=epochs,\n",
    "        device=device,                \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4b7c47",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e45c9",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f2e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(model, D=W*H, C=1, H=28, W=28, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_pixels = torch.zeros(1, D, 1).to(device) # Start with a blank image (all zeros)\n",
    "        h_prevs = model.initHiddenStates(1) # Initialize hidden states for a single image\n",
    "\n",
    "        for t in range(D): # Iterate through all D pixels\n",
    "            # Input for this step is the pixel generated in the previous step (or 0 for the first)\n",
    "            # The model's `step` function expects a batch dimension, so generated_pixels[:, t-1, :]\n",
    "            # or just generated_pixels[0, t-1, :].unsqueeze(0) for a single input\n",
    "            # For the first pixel (t=0), we feed a zero.\n",
    "            if t == 0:\n",
    "                current_pixel_input = torch.zeros(1, 1).to(device) # (B=1, input_size=1)\n",
    "            else:\n",
    "                current_pixel_input = generated_pixels[:, t-1, :] # Take the previously predicted pixel\n",
    "\n",
    "            # Get the prediction for the current pixel and update hidden states\n",
    "            predicted_pixel_value, h_prevs = model.step(current_pixel_input, h_prevs)\n",
    "\n",
    "            # Store the predicted pixel value. It's already (1, 1)\n",
    "            generated_pixels[0, t, :] = predicted_pixel_value.squeeze(0) # Store it in the sequence\n",
    "\n",
    "        # Reshape the generated sequence into an image\n",
    "        generated_image = generated_pixels.view(C, H, W)\n",
    "        return generated_image.cpu() # Move to CPU for plotting/saving\n",
    "\n",
    "generated_digit = generate_image(model, D=D, C=C, H=H, W=W, device=device)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(generated_digit.squeeze().numpy(), cmap='gray')\n",
    "plt.title(\"Generated Digit\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
