{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a7b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f982b7",
   "metadata": {},
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c94a06",
   "metadata": {},
   "source": [
    "<img src=\"./images/05.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3915b5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_08/utils.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import mlflow\n",
    "from torchinfo import summary\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from utils import train_network, set_seed, accuracy_score_wrapper, weight_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0575d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI'] = './mlruns08_5'\n",
    "mlflow.set_tracking_uri(os.environ.get('MLFLOW_TRACKING_URI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc06df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_07/mlruns07_1/143507330168611334', creation_time=1750415411076, experiment_id='143507330168611334', last_update_time=1750415411076, lifecycle_stage='active', name='Exercise07_1', tags={}>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow.set_experiment('Exercise08_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f59cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dfe04c",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "645553a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e2239",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = torchvision.datasets.MNIST(\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.MNIST(\"./data\", train=False, download=True, transform=transforms.ToTensor())\n",
    "mnist_train_loader = DataLoader(mnist_train, shuffle=True, batch_size=batch_size,)\n",
    "mnist_test_loader = DataLoader(mnist_test, batch_size=batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f776ad5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_train = torchvision.datasets.CIFAR10('./data', download=True, transform=transforms.ToTensor(), train=True,)\n",
    "cifar_test = torchvision.datasets.CIFAR10('./data', download=True, transform=transforms.ToTensor(), train=False,)\n",
    "\n",
    "cifar_train_loader = DataLoader(cifar_train, shuffle=True, batch_size=batch_size,)\n",
    "cifar_test_loader = DataLoader(cifar_test, batch_size=batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89b66e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 32, 32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(cifar_train[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64809262",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d3ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 32\n",
    "C = 3\n",
    "classes = 10\n",
    "leak_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17458533",
   "metadata": {},
   "source": [
    "### Residual without AdaptiveMaxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b72826",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlockE(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3, leak_rate=0.1):\n",
    "        \"\"\"\n",
    "        channels: how many channels are in the input/output to this layer\n",
    "        kernel_size: how large of a filter should we use\n",
    "        leak_rate: paramter for the LeakyReLU activation function\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #how much padding will our convolutional layers need to maintain the input shape\n",
    "        pad = (kernel_size-1)//2\n",
    "        \n",
    "        #Define the conv an BN layers we will use in a sub-network, just 2 hidden layers of conv/BN/activation\n",
    "        self.F = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size, padding=pad),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.LeakyReLU(leak_rate),\n",
    "            nn.Conv2d(channels, channels, kernel_size, padding=pad),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.LeakyReLU(leak_rate),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.F(x) #F() has all the work for the long path, we just add it to the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0351a97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBottleNeck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, leak_rate=0.1):\n",
    "        super().__init__()\n",
    "        #how much padding will our convolutional layers need to maintain the input shape\n",
    "        pad = (kernel_size-1)//2\n",
    "        #The botteneck should be smaller, so output/4 or input. You could also try changing max to min, its not a major issue. \n",
    "        bottleneck = max(out_channels//4, in_channels)\n",
    "        #Define the three sets of BN and convolution layers we need. \n",
    "        #Notice that for the 1x1 convs we use padding=0, because 1x1 will not change shape! \n",
    "        self.F = nn.Sequential(\n",
    "            #Compress down\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.LeakyReLU(leak_rate),\n",
    "            nn.Conv2d(in_channels, bottleneck, 1, padding=0),\n",
    "            #Normal layer doing a full conv\n",
    "            nn.BatchNorm2d(bottleneck),\n",
    "            nn.LeakyReLU(leak_rate),\n",
    "            nn.Conv2d(bottleneck, bottleneck, kernel_size, padding=pad),\n",
    "            #Expand back up\n",
    "            nn.BatchNorm2d(bottleneck),\n",
    "            nn.LeakyReLU(leak_rate),\n",
    "            nn.Conv2d(bottleneck, out_channels, 1, padding=0)\n",
    "        )\n",
    "\n",
    "        #By default, our shortcut will be the identiy function - which simply returns the input as the output\n",
    "        self.shortcut = nn.Identity()\n",
    "        #If we need to change the shape, then lets turn the shortcut into a small layer with 1x1 conv and BM\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut =  nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels, 1, padding=0), \n",
    "                    nn.BatchNorm2d(out_channels)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # shortcut(x) plays the role of \"x\", do as little work as possible to keep the tensor shapes the same.\n",
    "        return self.shortcut(x) + self.F(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a6f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_without_AdaptiveMaxPooling(D):\n",
    "    return nn.Sequential(\n",
    "        ResidualBottleNeck(C, n_filters), #BottleNeck to start because we need more channels. Its also common to start with just one normal hidden layer before starting residual blocks. \n",
    "        nn.LeakyReLU(leak_rate), #We are inserting a activation after each residual. This is optional. \n",
    "        ResidualBlockE(n_filters),\n",
    "        nn.LeakyReLU(leak_rate),\n",
    "        nn.MaxPool2d((2,2)),\n",
    "        ResidualBottleNeck(n_filters, 2*n_filters),\n",
    "        nn.LeakyReLU(leak_rate),\n",
    "        ResidualBlockE(2*n_filters),\n",
    "        nn.LeakyReLU(leak_rate),\n",
    "        nn.MaxPool2d((2,2)),\n",
    "        ResidualBottleNeck(2*n_filters, 4*n_filters),\n",
    "        nn.LeakyReLU(leak_rate),\n",
    "        ResidualBlockE(4*n_filters),\n",
    "        nn.LeakyReLU(leak_rate),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(D*n_filters//4, classes),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa13e7",
   "metadata": {},
   "source": [
    "### Residual with AdaptiveMaxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5d5a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_net= nn.Sequential(\n",
    "    ResidualBottleNeck(C, n_filters), #BottleNeck to start because we need more channels. Its also common to start with just one normal hidden layer before starting residual blocks. \n",
    "    nn.LeakyReLU(leak_rate), #We are inserting a activation after each residual. This is optional. \n",
    "    ResidualBlockE(n_filters),\n",
    "    nn.LeakyReLU(leak_rate),\n",
    "    nn.MaxPool2d((2,2)),\n",
    "    ResidualBottleNeck(n_filters, 2*n_filters),\n",
    "    nn.LeakyReLU(leak_rate),\n",
    "    ResidualBlockE(2*n_filters),\n",
    "    nn.LeakyReLU(leak_rate),\n",
    "    nn.MaxPool2d((2,2)),\n",
    "    ResidualBottleNeck(2*n_filters, 4*n_filters),\n",
    "    nn.LeakyReLU(leak_rate),\n",
    "    ResidualBlockE(4*n_filters),\n",
    "    nn.LeakyReLU(leak_rate),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cb4af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nn.Sequential(\n",
    "    nn.AdaptiveMaxPool2d((1, 1)),  # Output size 1x1 for each channel\n",
    "    nn.Flatten(),                  \n",
    "    nn.Linear(4*n_filters, 2*n_filters), \n",
    "    nn.LeakyReLU(leak_rate), \n",
    "    nn.Linear(2*n_filters, classes),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010aa0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_with_AdaptiveMaxPooling():\n",
    "    return nn.Sequential(\n",
    "        res_net,\n",
    "        classifier,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477ffb5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af53958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "epochs = 50\n",
    "params = {\n",
    "    'device': device,\n",
    "    'loss_func': loss_func.__class__.__name__,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd7efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_funcs = {\"Accuracy\": accuracy_score_wrapper}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55861dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'without_AdaptiveMaxPooling': resnet_without_AdaptiveMaxPooling,\n",
    "    'with_AdaptiveMaxPooling': resnet_with_AdaptiveMaxPooling\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d4cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = ('mnist', 'cifar')\n",
    "train_datasets = (mnist_train, cifar_train)\n",
    "train_dataloaders = (mnist_train_loader, cifar_train_loader)\n",
    "test_dataloaders = (mnist_test_loader, cifar_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f01e095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for experiment, model in models.items():\n",
    "    for i in range(len(data_names)):\n",
    "        train_loader = train_dataloaders[i]\n",
    "        valid_loader = test_dataloaders[i]\n",
    "        train_dataset = train_datasets[i]\n",
    "        data_name = data_names[i]\n",
    "        model.apply(weight_reset)\n",
    "        params['experiment'] = experiment\n",
    "        params['data_name'] = data_name\n",
    "        optimizer = optim.AdamW(model.parameters())\n",
    "        params['optimizer'] = optimizer.defaults\n",
    "        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_params_all = sum(p.numel() for p in model.parameters())\n",
    "        params['total_params'] = total_params\n",
    "        params['total_params_all'] = total_params_all\n",
    "        with open('model_summary.txt', 'w') as f:\n",
    "            f.write(str(summary(model, inpt_size=(batch_size,tuple(train_dataset[0][0].shape)))))\n",
    "        with mlflow.start_run(nested=True, run_name=f'{experiment}_{data_name}'):\n",
    "            mlflow.log_artifact('model_summary.txt')\n",
    "            mlflow.log_params(params)\n",
    "\n",
    "            results = train_network(\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                loss_func=loss_func,\n",
    "                train_loader=train_loader,\n",
    "                valid_loader=valid_loader,\n",
    "                epochs=epochs,\n",
    "                device=device,\n",
    "                score_funcs=score_funcs\n",
    "                # checkpoint_file_save='model.pth',\n",
    "                \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4ed78",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258ad739",
   "metadata": {},
   "source": [
    "How Does Performance Change?\n",
    "Replacing max-pooling with strided convolutions generally leads to several changes in performance, both in terms of model behavior and computational characteristics:\n",
    "\n",
    "1. Performance (Accuracy/Segmentation Quality):\n",
    "Improved Spatial Information Retention: This is the primary benefit. Max pooling is a destructive operation; it discards information by taking only the maximum value in a window. Strided convolutions, on the other hand, learn a transformation. This means the network can learn to extract more meaningful features during downsampling, leading to:\n",
    "Better Boundary Localization: Finer details are preserved, which is critical for accurate pixel-wise segmentation. The output masks tend to have sharper and more precise boundaries.\n",
    "Reduced \"Gridding\" Artifacts: In some upsampling architectures that rely heavily on ConvTranspose2d, replacing pooling with strided convolutions in the encoder can sometimes help reduce checkerboard artifacts in the output, although ConvTranspose2d itself can cause them.\n",
    "Increased Model Capacity/Complexity: Strided convolutions introduce learnable parameters (weights and biases) where max pooling did not. This increases the model's capacity, allowing it to learn more complex representations. However, this also means:\n",
    "Potentially More Prone to Overfitting: With more parameters, the model might be more susceptible to overfitting if the dataset is small or regularization is insufficient.\n",
    "Requires More Data: To effectively train these additional parameters, a larger and more diverse dataset might be beneficial.\n",
    "2. Computational Performance:\n",
    "Increased Computational Cost:\n",
    "More FLOPS (Floating Point Operations): Convolutions are computationally more expensive than simple max-pooling operations. Even though they reduce spatial dimensions, the multiplication and addition operations involved in the convolutions will increase the overall FLOP count per inference.\n",
    "Increased Memory Usage: While not drastically different, the intermediate feature maps might require slightly more memory depending on the exact kernel and stride choices compared to max-pooling.\n",
    "Potentially Slower Training/Inference: Due to the increased computational cost, training and inference times will likely increase compared to a max-pooling based U-Net of similar depth.\n",
    "No \"Indifference to Small Translations\": Max pooling offers a degree of translational invariance because a small shift in the input might not change the maximum value in a pool. Strided convolutions are more sensitive to input translations, which could be both a benefit (more precise localization) and a drawback (less robust to minor input variations without proper augmentation).\n",
    "How to \"Measure\" the Change:\n",
    "To quantify the performance change, you would need to:\n",
    "\n",
    "Implement both versions: One with max-pooling and one with strided convolutions.\n",
    "Train both models: On the same dataset, using the same training pipeline (optimizers, learning rates, epochs, etc.).\n",
    "Evaluate both models: Using appropriate metrics for your task (e.g., IoU, Dice Score, Pixel Accuracy for segmentation) on a held-out test set.\n",
    "Compare computational resources: Monitor training time, inference time, and GPU memory usage.\n",
    "In summary:\n",
    "\n",
    "Replacing max-pooling with strided convolutions in a U-Net often leads to improved segmentation accuracy, particularly for boundary delineation, due to better spatial information preservation and increased model learning capacity. However, this comes at the cost of increased computational complexity (more FLOPS, potentially longer training/inference times). The trade-off is often worthwhile for tasks demanding high spatial precision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
