{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09c9f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a4358d",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e20d855",
   "metadata": {},
   "source": [
    "<img src=\"./images/06.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36bf28c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_05/utils.py:6: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import torch.nn as nn \n",
    "from typing import DefaultDict, Any, Callable, Optional\n",
    "import mlflow\n",
    "import os\n",
    "from utils import train_network, accuracy_score_wrapper\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "import mlflow\n",
    "from  sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12f5ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI'] = './mlruns05_6'\n",
    "mlflow.set_tracking_uri(os.environ.get('MLFLOW_TRACKING_URI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c1641b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_05/mlruns05_6/679868600860297786', creation_time=1749579304858, experiment_id='679868600860297786', last_update_time=1749579304858, lifecycle_stage='active', name='Exercise05_6', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment('Exercise05_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e3ccd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic=True\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e283ecd",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af750119",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.FashionMNIST(\"./data\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = torchvision.datasets.FashionMNIST(\"./data\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "train_sub_set, valid_sub_set = train_test_split(\n",
    "    np.arange(len(train_data)),\n",
    "    test_size=0.1,\n",
    "    shuffle=True, \n",
    "    random_state=42,\n",
    "    stratify=train_data.targets)\n",
    "\n",
    "train_dataset = Subset(train_data, train_sub_set)\n",
    "valid_dataset = Subset(train_data, valid_sub_set)\n",
    "batch = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch)\n",
    "test_loader = DataLoader(test_data, batch_size=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ab6959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "score_funcs = {\"Accuracy\": accuracy_score_wrapper}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e211a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "params = {\n",
    "    'device': device,\n",
    "    'loss_func': loss_func.__class__.__name__,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch\n",
    "}\n",
    "D = 28*28 #28 * 28 images \n",
    "C = 1\n",
    "W = 28\n",
    "H = 28\n",
    "classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec7fc5d",
   "metadata": {},
   "source": [
    "## Hyperparameter Tunning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47060841",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = {\n",
    "'ReLU': nn.ReLU(),\n",
    "'Tanh': nn.Tanh(),\n",
    "'LeakyReLU': nn.LeakyReLU(),\n",
    "'Sigmoid': nn.Sigmoid()\n",
    "}\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "def champion_callback(study, frozen_trial):\n",
    "    winner = study.user_attrs.get('winner', None)\n",
    "    \n",
    "    if winner is None:\n",
    "        print(f'Initial trial {frozen_trial.number} achived value: {frozen_trial.value}')\n",
    "    elif winner != study.best_value and study.best_value:   # second condition is for preventing zero devision\n",
    "        improvment_percent = (abs(winner - study.best_value) / abs(study.best_value)) * 100\n",
    "        print(f'Trial {frozen_trial.number} achived value: {frozen_trial.value} with {improvment_percent:.4f}% improvment')\n",
    "    study.set_user_attr('winner', study.best_value)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "    'hidden_layers': trial.suggest_int('hidden_layers', 1, 5),\n",
    "    'hidden_neurons': trial.suggest_categorical('hidden_neurons', [2**i for i in range(6,9)]),\n",
    "    'activation': trial.suggest_categorical(\"activation\", list(activation_functions.keys())),\n",
    "    'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1, log=True)\n",
    "    }\n",
    "    activation = activation_functions[params['activation']]\n",
    "    hidden_neurons = params['hidden_neurons']\n",
    "    hidden_layers = params['hidden_layers']\n",
    "    layers = [nn.Flatten(),\n",
    "        nn.Linear(D, hidden_neurons ),\n",
    "        activation]\n",
    "    for _ in range(hidden_layers-1):\n",
    "        layers.extend([nn.Linear(hidden_neurons,  hidden_neurons), activation])\n",
    "\n",
    "    layers.append(nn.Linear(hidden_neurons, classes),)\n",
    "    fc_model = nn.Sequential(*layers)\n",
    "\n",
    "    # run_name = f'trial_lr_{params[\"learning_rate\"]:.8f}'\n",
    "    run_name = f'trial: {trial.number}'\n",
    "    with mlflow.start_run(nested=True, run_name=run_name) as run:\n",
    "        trial.set_user_attr('mlflow_run_id', run.info.run_id)\n",
    "        optimizer = torch.optim.SGD(fc_model.parameters(), lr=params['learning_rate'])\n",
    "        params['optimizer'] = optimizer.defaults\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        with open (\"model_summary.txt\", \"w\") as f:\n",
    "            f.write(str(summary(fc_model, input_size=(batch, C, W, H))))\n",
    "        mlflow.log_artifact(\"model_summary.txt\")\n",
    "\n",
    "        # model.apply(weight_reset)\n",
    "        cnn_results = train_network(\n",
    "            model=fc_model,\n",
    "            loss_func=loss_func,\n",
    "            train_loader=train_loader,\n",
    "            valid_loader=valid_loader,\n",
    "            # test_loader=test_loader\n",
    "            epochs=epochs,\n",
    "            optimizer=optimizer,\n",
    "            score_funcs=score_funcs,\n",
    "            device=device,\n",
    "            disable_tqdm=True,\n",
    "            # checkpont_file_save='model.pth'\n",
    "        )\n",
    "    return  cnn_results['valid Acc'].iloc[-1]\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20, callbacks=[champion_callback])\n",
    "champion_trial = study.best_trial\n",
    "champion_run_id = champion_trial.user_attrs('champion_run_id')\n",
    "if champion_run_id:\n",
    "    mlflow_client = mlflow.tracking.MlflowClient()\n",
    "    mlflow_client.set_tag(champion_run_id, \"is_champion\", \"true\")\n",
    "    mlflow_client.set_tag(champion_run_id, \"champion_metric_value\", str(champion_trial.value))\n",
    "    mlflow_client.set_tag(champion_run_id, \"optuna_trial_number\", str(champion_trial.number))\n",
    "    print(f\"Champion trial: {champion_trial.number} with value {champion_trial.value}\")\n",
    "else:\n",
    "    print(\"Error: Could not retrieve champion_run_id from champion_trial.user_attrs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdadb99",
   "metadata": {},
   "source": [
    "## Traning with Selected Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14d21f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "run_id = 'fd795a161d2e43aea9747b534d262842' #trial: 3\n",
    "run = mlflow.get_run(run_id)\n",
    "params = run.data.params\n",
    "\n",
    "activation_functions = {\n",
    "'ReLU': nn.ReLU(),\n",
    "'Tanh': nn.Tanh(),\n",
    "'LeakyReLU': nn.LeakyReLU(),\n",
    "'Sigmoid': nn.Sigmoid()\n",
    "}\n",
    "hidden_layers = int(params['hidden_layers'])\n",
    "hidden_neurons = int(params['hidden_neurons'])\n",
    "activation = activation_functions[params['activation']]\n",
    "optimizer_dict = ast.literal_eval( params['optimizer'])\n",
    "learning_rate = float(optimizer_dict['lr'])\n",
    "\n",
    "layers = [nn.Flatten(),\n",
    "    nn.Linear(D, hidden_neurons ),\n",
    "    activation]\n",
    "for _ in range(hidden_layers):\n",
    "    layers.extend([nn.Linear(hidden_neurons,  hidden_neurons), activation])\n",
    "\n",
    "layers.append(nn.Linear(hidden_neurons, classes),)\n",
    "fc_model = nn.Sequential(*layers)\n",
    "\n",
    "with mlflow.start_run(nested=True, run_name='final_run'):\n",
    "    mlflow.set_tag(\"final_run\", \"True\")\n",
    "    optimizer = torch.optim.SGD(fc_model.parameters(), lr=learning_rate)\n",
    "    params['optimizer'] = optimizer.defaults\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    with open (\"model_summary.txt\", \"w\") as f:\n",
    "        f.write(str(summary(fc_model, input_size=(batch, C, W, H))))\n",
    "    mlflow.log_artifact(\"model_summary.txt\")\n",
    "\n",
    "    # model.apply(weight_reset)\n",
    "    results = train_network(\n",
    "        model=fc_model,\n",
    "        loss_func=loss_func,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        # test_loader=test_loader\n",
    "        epochs=epochs,\n",
    "        optimizer=optimizer,\n",
    "        score_funcs=score_funcs,\n",
    "        device=device,\n",
    "        checkpoint_file_save='final_model.pth'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4844e095",
   "metadata": {},
   "source": [
    "<img src=\"./images/E6_train_acc.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb135e5",
   "metadata": {},
   "source": [
    "<img src=\"./images/E6_train_loss.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82ec72b",
   "metadata": {},
   "source": [
    "<img src=\"./images/E6_valid_acc.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f0154",
   "metadata": {},
   "source": [
    "<img src=\"./images/E6_valid_loss.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045c7c06",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c3471f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_mlflow(\n",
    "    run_id, artifact_path, model, device\n",
    "):\n",
    "    artifact_uri = f'runs:/{run_id}/{artifact_path}'\n",
    "    checkpoint_path = mlflow.artifacts.download_artifacts(artifact_uri=artifact_uri)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # optimizer_state_dict = checkpoint['optimizer_state_dict']\n",
    "    results = checkpoint['results']\n",
    "    epoch = checkpoint['epoch']\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return model, results, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "433edfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1861/3900418335.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "filter_string = \"tags.final_run = 'True'\"\n",
    "runs_list = client.search_runs(\n",
    "    experiment_ids='679868600860297786',\n",
    "    filter_string=filter_string,\n",
    "    max_results=1,)\n",
    "\n",
    "run_id = runs_list[0].info.run_id\n",
    "run = mlflow.get_run(run_id)\n",
    "params = run.data.params\n",
    "activation_functions = {\n",
    "'ReLU': nn.ReLU(),\n",
    "'Tanh': nn.Tanh(),\n",
    "'LeakyReLU': nn.LeakyReLU(),\n",
    "'Sigmoid': nn.Sigmoid()\n",
    "}\n",
    "hidden_layers = int(params['hidden_layers'])\n",
    "hidden_neurons = int(params['hidden_neurons'])\n",
    "activation = activation_functions[params['activation']]\n",
    "optimizer_dict = ast.literal_eval( params['optimizer'])\n",
    "learning_rate = float(optimizer_dict['lr'])\n",
    "\n",
    "layers = [nn.Flatten(),\n",
    "    nn.Linear(D, hidden_neurons ),\n",
    "    activation]\n",
    "for _ in range(hidden_layers):\n",
    "    layers.extend([nn.Linear(hidden_neurons,  hidden_neurons), activation])\n",
    "\n",
    "layers.append(nn.Linear(hidden_neurons, classes),)\n",
    "fc_model = nn.Sequential(*layers)\n",
    "artifact_path = 'final_model.pth'\n",
    "model, results, epoch = load_model_from_mlflow(\n",
    "        run_id=run_id,\n",
    "        artifact_path=artifact_path,\n",
    "        model=fc_model, \n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7bc2e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "score_funcs = {\"Accuracy\": accuracy_score_wrapper}\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    running_loss = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for inputs, labels in tqdm(test_loader, desc='tetsing', leave=False):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        y_hat = model(inputs)\n",
    "        loss = loss_func(y_hat, labels)\n",
    "        running_loss.append(loss.item())\n",
    "\n",
    "        if score_funcs is not None:\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "            y_hat = y_hat.detach().cpu().numpy()\n",
    "            y_true.extend(labels)\n",
    "            y_pred.extend(y_hat)\n",
    "\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    y_true = np.asanyarray(y_true)\n",
    "\n",
    "    if score_funcs is not None and len(score_funcs) > 0:\n",
    "        for score_name , score_func in score_funcs.items():\n",
    "            score_value = score_func(y_pred, y_true)\n",
    "            print(f'{score_name} = {score_value}')\n",
    "            client.set_tag(run_id, f'test_{score_name}', score_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ee03717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8743"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_func(y_pred, y_true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
