{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50bef5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b462bf",
   "metadata": {},
   "source": [
    "# Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c4a9f",
   "metadata": {},
   "source": [
    "<img src=\"./images/07.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f24fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import torch.nn as nn \n",
    "from typing import DefaultDict, Any, Callable, Optional\n",
    "import mlflow\n",
    "import os\n",
    "from utils import train_network, accuracy_score_wrapper\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "import mlflow\n",
    "from  sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cdf4b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI'] = './mlruns05_7'\n",
    "mlflow.set_tracking_uri(os.environ.get('MLFLOW_TRACKING_URI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96248ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/12 18:34:32 INFO mlflow.tracking.fluent: Experiment with name 'Exercise05_7' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/spakdel/my_projects/Books/Inside-Deep-Learning/Exercises_InsideDeepLearning/Chapter_05/mlruns05_7/500994595681244077', creation_time=1749740672239, experiment_id='500994595681244077', last_update_time=1749740672239, lifecycle_stage='active', name='Exercise05_7', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment('Exercise05_7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a9f7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic=True\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9287f32a",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d48ef6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.FashionMNIST(\"./data\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = torchvision.datasets.FashionMNIST(\"./data\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "train_sub_set, valid_sub_set = train_test_split(\n",
    "    np.arange(len(train_data)),\n",
    "    test_size=0.1,\n",
    "    shuffle=True, \n",
    "    random_state=42,\n",
    "    stratify=train_data.targets)\n",
    "\n",
    "train_dataset = Subset(train_data, train_sub_set)\n",
    "valid_dataset = Subset(train_data, valid_sub_set)\n",
    "batch = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch)\n",
    "test_loader = DataLoader(test_data, batch_size=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42fb705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "score_funcs = {\"Accuracy\": accuracy_score_wrapper}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da92795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "params = {\n",
    "    'device': device,\n",
    "    'loss_func': loss_func.__class__.__name__,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601a8788",
   "metadata": {},
   "source": [
    "## Hyperparameter Tunning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cda10fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 28\n",
    "H = 28\n",
    "C = 1\n",
    "classes = 10\n",
    "filter = 16\n",
    "K = 3\n",
    "def build_model(num_conv_layers,\n",
    "                num_pool_layers,\n",
    "                num_hidden_layer=2, \n",
    "                init_hidden_size=512, \n",
    "                decay_factor=2,\n",
    "                activation=nn.ReLU()):\n",
    "    layers =[]\n",
    "    in_channels = C\n",
    "    out_channels = 32\n",
    "    if num_pool_layers:\n",
    "        pool_interval = max(1, num_conv_layers // (num_pool_layers + 1))\n",
    "    else:\n",
    "        pool_interval = num_conv_layers + 1\n",
    "    \n",
    "    currnet_pool_rounds = 0\n",
    "    for i in range(num_conv_layers):\n",
    "        layers.append(nn.Conv2d(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=3//2))\n",
    "        layers.append(activation)\n",
    "        in_channels = out_channels\n",
    "        if (i+1) % pool_interval == 0 and currnet_pool_rounds < num_pool_layers:\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "            currnet_pool_rounds += 1\n",
    "            out_channels *= 2\n",
    "    final_w = W // (2 ** num_pool_layers)\n",
    "    final_h = H // (2 ** num_pool_layers)\n",
    "    fc_layers = []\n",
    "    # Compute the number of features after flattening.\n",
    "    in_features = in_channels * final_w * final_h\n",
    "\n",
    "    fc_layers.append(nn.Flatten())\n",
    "\n",
    "    if num_hidden_layer == 0:\n",
    "        # Directly classify without extra hidden layers.\n",
    "        fc_layers.append(nn.Linear(in_features, classes))\n",
    "    else:\n",
    "        # First FC layer: from flattened output to initial hidden size.\n",
    "        fc_layers.append(nn.Linear(in_features, init_hidden_size))\n",
    "        fc_layers.append(activation)\n",
    "        \n",
    "        # Set the current hidden size that will be reduced in subsequent layers.\n",
    "        current_hidden_size = init_hidden_size\n",
    "\n",
    "        # Add additional hidden layers with decreasing size.\n",
    "        for layer in range(1, num_hidden_layer):\n",
    "            # Compute new hidden size with decay.\n",
    "            new_hidden_size = max(10, current_hidden_size // decay_factor)\n",
    "            fc_layers.append(nn.Linear(current_hidden_size, new_hidden_size))\n",
    "            fc_layers.append(activation)\n",
    "            current_hidden_size = new_hidden_size\n",
    "\n",
    "        # Final classification layer from the last hidden dimension to the number of classes.\n",
    "        fc_layers.append(nn.Linear(current_hidden_size, classes))\n",
    "\n",
    "    classifier = nn.Sequential(*fc_layers)\n",
    "    model = nn.Sequential(*layers, classifier)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = {\n",
    "'ReLU': nn.ReLU(),\n",
    "'Tanh': nn.Tanh(),\n",
    "'LeakyReLU': nn.LeakyReLU(),\n",
    "'Sigmoid': nn.Sigmoid()\n",
    "}\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "def champion_callback(study, frozen_trial):\n",
    "    winner = study.user_attrs.get('winner', None)\n",
    "    if winner is None:\n",
    "        print(f'Initial trial {frozen_trial.number} achived value: {frozen_trial.value}')\n",
    "    elif winner != study.best_value and study.best_value:   # second condition is for preventing zero devision\n",
    "        improvment_percent = (abs(winner - study.best_value) / abs(study.best_value)) * 100\n",
    "        print(f'Trial {frozen_trial.number} achived value: {frozen_trial.value} with {improvment_percent:.4f}% improvment')\n",
    "    study.set_user_attr('winner', study.best_value)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "    'learning_rate': trial.suggest_float('lr', 1e-4, 1, log=True),\n",
    "    'num_conv': trial.suggest_int(\"num_conv\", 2, 10),\n",
    "    'num_pool': trial.suggest_int(\"num_pool\", 0, 2),\n",
    "    'activation': trial.suggest_categorical(\"activation\", list(activation_functions.keys())),\n",
    "    'fc_hidden_layer': trial.suggest_int('fc_hidden_layer', 0, 5),\n",
    "    'init_hidden_size':  trial.suggest_categorical('init_hidden_size', [2**i for i in range(6,10)]),\n",
    "    }\n",
    "    cnn_model = build_model(\n",
    "        num_conv_layers=params['num_conv'],\n",
    "        num_pool_layers=params['num_pool'],\n",
    "        num_hidden_layer=params['fc_hidden_layer'], \n",
    "        init_hidden_size=params['init_hidden_size'], \n",
    "        activation=activation_functions[params['activation']],\n",
    "        )\n",
    "    # run_name = f'trial_lr_{params[\"learning_rate\"]:.8f}'\n",
    "    run_name = f'trial: {trial.number}'\n",
    "    with mlflow.start_run(nested=True, run_name=run_name) as run:\n",
    "        trial.set_user_attr('mlflow_run_id', run.info.run_id)\n",
    "        \n",
    "        optimizer = torch.optim.SGD(cnn_model.parameters(), lr=params['learning_rate'])\n",
    "        params['optimizer'] = optimizer.defaults\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        with open (\"model_summary.txt\", \"w\") as f:\n",
    "            f.write(str(summary(cnn_model, input_size=(batch, C, W, H))))\n",
    "        mlflow.log_artifact(\"model_summary.txt\")\n",
    "\n",
    "        # model.apply(weight_reset)\n",
    "        cnn_results = train_network(\n",
    "            model=cnn_model,\n",
    "            loss_func=loss_func,\n",
    "            train_loader=train_loader,\n",
    "            valid_loader=valid_loader,\n",
    "            # test_loader=test_loader\n",
    "            epochs=epochs,\n",
    "            optimizer=optimizer,\n",
    "            score_funcs=score_funcs,\n",
    "            device=device,\n",
    "            disable_tqdm=True,\n",
    "            # checkpont_file_save='model.pth'\n",
    "        )\n",
    "    return  cnn_results['valid Accuracy'].iloc[-1]\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=40, callbacks=[champion_callback])\n",
    "champion_trial = study.best_trial\n",
    "champion_run_id = champion_trial.user_attrs('champion_run_id')\n",
    "if champion_run_id:\n",
    "    mlflow_client = mlflow.tracking.MlflowClient()\n",
    "    mlflow_client.set_tag(champion_run_id, \"is_champion\", \"true\")\n",
    "    mlflow_client.set_tag(champion_run_id, \"champion_metric_value\", str(champion_trial.value))\n",
    "    mlflow_client.set_tag(champion_run_id, \"optuna_trial_number\", str(champion_trial.number))\n",
    "    print(f\"Champion trial: {champion_trial.number} with value {champion_trial.value}\")\n",
    "else:\n",
    "    print(\"Error: Could not retrieve champion_run_id from champion_trial.user_attrs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0379b6da",
   "metadata": {},
   "source": [
    "## Traning with Selected Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6ef3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \n",
    "run = mlflow.get_run(run_id)\n",
    "parameters = run.data.params\n",
    "learning_rate = parameters['learning_rate']\n",
    "num_conv = parameters['num_conv']\n",
    "num_pool = parameters['num_pool']\n",
    "activation = parameters['activation']\n",
    "fc_hidden_layer = parameters['fc_hidden_layer']\n",
    "init_hidden_size = parameters['init_hidden_size']\n",
    "\n",
    "cnn_model = build_model(\n",
    "        num_conv_layers=num_conv,\n",
    "        num_pool_layers=params['num_pool'],\n",
    "        num_hidden_layer=params['fc_hidden_layer'], \n",
    "        init_hidden_size=params['init_hidden_size'], \n",
    "        activation=activation_functions[params['activation']],\n",
    "        )\n",
    "params['hidden_layers'] = hidden_layers\n",
    "params['hidden_neurons'] = hidden_neurons\n",
    "params['learning_rate'] = learning_rate\n",
    "with mlflow.start_run(nested=True, run_name='final_run'):\n",
    "\n",
    "    optimizer = torch.optim.SGD(fc_model.parameters(), lr=learning_rate)\n",
    "    params['optimizer'] = optimizer.defaults\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    with open (\"model_summary.txt\", \"w\") as f:\n",
    "        f.write(str(summary(fc_model)))\n",
    "    mlflow.log_artifact(\"model_summary.txt\")\n",
    "\n",
    "    # model.apply(weight_reset)\n",
    "    results = train_network(\n",
    "        model=fc_model,\n",
    "        loss_func=loss_func,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        # test_loader=test_loader\n",
    "        epochs=epochs,\n",
    "        optimizer=optimizer,\n",
    "        score_funcs=score_funcs,\n",
    "        device=device,\n",
    "        checkpoint_file_save='final_model.pth'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054eba5e",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e95f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_mlflow(\n",
    "    run_id, artifact_path, model, device\n",
    "):\n",
    "    artifact_uri = f'runs:/{run_id}/{artifact_path}'\n",
    "    checkpoint_path = mlflow.artifacts.download_artifacts(artifact_uri=artifact_uri)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # optimizer_state_dict = checkpoint['optimizer_state_dict']\n",
    "    results = checkpoint['results']\n",
    "    epoch = checkpoint['epoch']\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return model, results, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac611bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \n",
    "run = mlflow.get_run(run_id)\n",
    "parameters = run.data.params\n",
    "hidden_layers = parameters['hidden_layers']\n",
    "hidden_neurons = parameters['hidden_neurons']\n",
    "activation = parameters['activation']\n",
    "learning_rate = parameters['learning_rate']\n",
    "layers = [nn.Flatten(),\n",
    "    nn.Linear(D, hidden_neurons ),\n",
    "    activation]\n",
    "for _ in range(hidden_layers):\n",
    "    layers.extend([nn.Linear(hidden_neurons,  hidden_neurons), activation])\n",
    "\n",
    "layers.append(nn.Linear(hidden_neurons, classes),)\n",
    "fc_model = nn.Sequential(*layers)\n",
    "optimizer = torch.optim.SGD(fc_model.parameters(), lr=learning_rate)\n",
    "artifact_path = \n",
    "model, results, epoch = load_model_from_mlflow(\n",
    "        run_id=run_id,\n",
    "        artifact_path=artifact_path,\n",
    "        model=fc_model,  # Replace with your actual model class\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac547fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_funcs = {\"Accuracy\": accuracy_score_wrapper}\n",
    "results = defaultdict(list)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    running_loss = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for inputs, labels in tqdm(test_loader, desc='tetsing', leave=False):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        y_hat = model(inputs)\n",
    "        loss = loss_func(y_hat, labels)\n",
    "        running_loss.append(loss.item())\n",
    "\n",
    "        if score_funcs is not None:\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "            y_hat = y_hat.detach().cpu().numpy()\n",
    "            y_true.extend(labels)\n",
    "            y_pred.extend(y_hat)\n",
    "\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    y_true = np.asanyarray(y_true)\n",
    "\n",
    "    if score_funcs is not None and len(score_funcs) > 0:\n",
    "        for score_name , score_func in score_funcs.items():\n",
    "            print(f'{score_name} = {score_func(y_pred, y_true)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
